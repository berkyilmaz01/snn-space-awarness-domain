#!/usr/bin/env python3
"""
SpikeSEG Evaluation Script with HULK-SMASH Instance Segmentation

Evaluates trained SpikeSEG model on EBSSA dataset using paper metrics:
- Informedness = Sensitivity + Specificity - 1
- Sensitivity = TP / (TP + FN)  (true positive rate)
- Specificity = TN / (TN + FP)  (true negative rate)

Implements the full pipeline from IGARSS 2023:
1. Encoder: Extract STDP features and classification spikes
2. HULK: Unravel spikes back to pixel space using decoder with unpooling
3. SMASH: Group instances by featural-temporal similarity + spatial proximity
4. Evaluation: Compare against ground truth with 1-pixel spatial tolerance

Paper Reference:
    Kirkland et al. 2023 (IGARSS) - "Neuromorphic sensing and processing
    for space domain awareness" reports 89.1% informedness on EBSSA.

Usage:
    python scripts/evaluate.py --checkpoint runs/experiment/checkpoints/checkpoint_best.pt
"""

import argparse
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from scipy import ndimage

from spikeseg.data.datasets import EBSSADataset
from spikeseg.models.encoder import SpikeSEGEncoder, EncoderConfig, LayerConfig, EncoderOutput
from spikeseg.algorithms import HULKDecoder, group_instances_to_objects, Instance, BoundingBox, Object


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%H:%M:%S"
)
logger = logging.getLogger(__name__)


def dilate_mask(mask: torch.Tensor, pixels: int = 1) -> torch.Tensor:
    """
    Dilate a binary mask by a given number of pixels.

    Used to implement 1-pixel spatial tolerance from paper:
    "A correct prediction was determined if the centroid of the detection
    lands within 1 pixel of the centroid of the ground-truth"

    Args:
        mask: Binary mask (H, W) or (B, H, W)
        pixels: Dilation amount (default 1 for paper methodology)

    Returns:
        Dilated mask with same shape as input
    """
    if pixels == 0:
        return mask

    # Add batch and channel dims for max_pool2d
    needs_batch = mask.dim() == 2
    if needs_batch:
        mask = mask.unsqueeze(0).unsqueeze(0)
    elif mask.dim() == 3:
        mask = mask.unsqueeze(1)

    # Dilate using max pooling with padding
    kernel_size = 2 * pixels + 1
    dilated = F.max_pool2d(
        mask.float(),
        kernel_size=kernel_size,
        stride=1,
        padding=pixels
    )

    # Remove added dimensions
    if needs_batch:
        dilated = dilated.squeeze(0).squeeze(0)
    elif mask.dim() == 4:
        dilated = dilated.squeeze(1)

    return dilated


def extract_gt_centroids(label: torch.Tensor, use_all_points: bool = True) -> List[Tuple[float, float]]:
    """
    Extract ground truth object positions from a label mask.

    For satellite detection, the label mask often forms a trail/streak.
    Using a single centroid (center of trail) is WRONG because detections
    may be at one end of the trail, far from the center.

    Args:
        label: Binary label mask (H, W)
        use_all_points: If True, return all nonzero pixel locations (for trail matching).
                       If False, return connected component centroids (legacy behavior).

    Returns:
        List of (x, y) coordinates. If use_all_points=True, returns sampled points
        along the trail. If False, returns centroids of connected components.
    """
    label_np = label.cpu().numpy().astype(np.uint8)

    if use_all_points:
        # Return sampled points along the trail (not centroid)
        # This allows matching detections anywhere along the satellite trajectory
        coords = np.where(label_np > 0)
        if len(coords[0]) == 0:
            return []

        # Sample points to avoid returning thousands of pixels
        # Use every 10th pixel to get trajectory points
        n_points = len(coords[0])
        if n_points > 20:
            step = max(1, n_points // 20)
            indices = np.arange(0, n_points, step)
            return [(coords[1][i], coords[0][i]) for i in indices]  # (x, y) format
        else:
            return [(coords[1][i], coords[0][i]) for i in range(n_points)]

    # Legacy: connected component centroids
    labeled_array, num_features = ndimage.label(label_np)

    centroids = []
    for i in range(1, num_features + 1):
        # Get coordinates of this component
        coords = np.where(labeled_array == i)
        if len(coords[0]) > 0:
            # Centroid = mean of coordinates
            cy = coords[0].mean()
            cx = coords[1].mean()
            centroids.append((cx, cy))

    return centroids


def extract_detection_centroids(objects: List[Object]) -> List[Tuple[float, float]]:
    """
    Extract detection centroids from SMASH objects.

    Args:
        objects: List of Object from SMASH grouping

    Returns:
        List of (x, y) centroid coordinates
    """
    centroids = []
    for obj in objects:
        if obj.combined_bbox is not None:
            # Use bbox center as centroid (BoundingBox uses x_min/x_max/y_min/y_max)
            cx = (obj.combined_bbox.x_min + obj.combined_bbox.x_max) / 2
            cy = (obj.combined_bbox.y_min + obj.combined_bbox.y_max) / 2
            centroids.append((cx, cy))
        elif obj.instances:
            # Fall back to instance mask/bbox centroid
            for inst in obj.instances:
                if hasattr(inst, 'bbox') and inst.bbox is not None:
                    cx = (inst.bbox.x_min + inst.bbox.x_max) / 2
                    cy = (inst.bbox.y_min + inst.bbox.y_max) / 2
                    centroids.append((cx, cy))
                    break
                elif hasattr(inst, 'mask') and inst.mask is not None:
                    mask = inst.mask
                    if mask.sum() > 0:
                        coords = torch.where(mask > 0)
                        cy = coords[0].float().mean().item()
                        cx = coords[1].float().mean().item()
                        centroids.append((cx, cy))
                        break

    return centroids


def compute_object_metrics(
    detection_centroids: List[Tuple[float, float]],
    gt_points: List[Tuple[float, float]],
    tolerance: float = 1.0,
    trajectory_mode: bool = True
) -> Dict[str, int]:
    """
    Compute object-level detection metrics using centroid matching.

    Paper methodology (IGARSS 2023):
    "A correct prediction was determined if the centroid of the detection
    lands within 1 pixel of the centroid of the ground-truth"

    IMPORTANT: For satellite detection, GT is often a trajectory (trail of points).
    In trajectory_mode, we check if detection is near ANY point on the trajectory,
    and count ONE TP per trajectory if any detection matches.

    Args:
        detection_centroids: List of (x, y) from detected objects
        gt_points: List of (x, y) from ground truth. In trajectory_mode, these
                  are points along the satellite trajectory.
        tolerance: Maximum distance for a match (default 1 pixel)
        trajectory_mode: If True, GT points are from ONE object's trajectory.
                        A detection matching ANY point counts as 1 TP.
                        If False, each GT point is a separate object.

    Returns:
        Dictionary with tp, fp, fn counts
    """
    n_detections = len(detection_centroids)
    n_gt_points = len(gt_points)

    if n_detections == 0 and n_gt_points == 0:
        return {'tp': 0, 'fp': 0, 'fn': 0}

    if n_detections == 0:
        # No detections, 1 FN (missed the trajectory/object)
        return {'tp': 0, 'fp': 0, 'fn': 1 if trajectory_mode else n_gt_points}

    if n_gt_points == 0:
        # No GT, all detections are FP
        return {'tp': 0, 'fp': n_detections, 'fn': 0}

    if trajectory_mode:
        # Trajectory mode: GT points are from ONE object (satellite trajectory)
        # Check if ANY detection is within tolerance of ANY trajectory point
        # This gives: 1 TP if matched, else 1 FN
        # FP = detections that don't match any trajectory point

        matched_det = set()

        for det_idx, (dx, dy) in enumerate(detection_centroids):
            # Check if this detection is near any trajectory point
            for (gx, gy) in gt_points:
                dist = np.sqrt((dx - gx) ** 2 + (dy - gy) ** 2)
                if dist <= tolerance:
                    matched_det.add(det_idx)
                    break  # Found a match, no need to check more GT points

        # If any detection matched, it's 1 TP for the object
        # If no detection matched, it's 1 FN
        has_match = len(matched_det) > 0
        tp = 1 if has_match else 0
        fn = 0 if has_match else 1
        fp = n_detections - len(matched_det)  # Detections that didn't match

        return {'tp': tp, 'fp': fp, 'fn': fn}

    else:
        # Legacy mode: each GT point is a separate object
        matched_gt = set()
        matched_det = set()

        # Greedy matching: for each detection, find closest GT within tolerance
        for det_idx, (dx, dy) in enumerate(detection_centroids):
            best_dist = float('inf')
            best_gt_idx = None

            for gt_idx, (gx, gy) in enumerate(gt_points):
                if gt_idx in matched_gt:
                    continue

                dist = np.sqrt((dx - gx) ** 2 + (dy - gy) ** 2)
                if dist <= tolerance and dist < best_dist:
                    best_dist = dist
                    best_gt_idx = gt_idx

            if best_gt_idx is not None:
                matched_gt.add(best_gt_idx)
                matched_det.add(det_idx)

        tp = len(matched_det)
        fp = n_detections - tp
        fn = n_gt_points - len(matched_gt)

        return {'tp': tp, 'fp': fp, 'fn': fn}


def compute_metrics(
    predictions: torch.Tensor,
    labels: torch.Tensor,
    spatial_tolerance: int = 1
) -> Dict[str, float]:
    """
    Compute segmentation metrics for binary classification with spatial tolerance.

    Paper methodology (IGARSS 2023):
    "A correct prediction was determined if the centroid of the detection
    lands within 1 pixel of the centroid of the ground-truth"

    This is implemented by dilating the label mask before comparison,
    which allows predictions within 1 pixel of ground truth to count as TP.

    Args:
        predictions: Predicted class labels (B, H, W) or (H, W)
        labels: Ground truth labels (B, H, W) or (H, W)
        spatial_tolerance: Pixels of tolerance (default 1 per paper)

    Returns:
        Dictionary with TP, TN, FP, FN, sensitivity, specificity, informedness
    """
    # Dilate labels for spatial tolerance
    labels_dilated = dilate_mask(labels, spatial_tolerance) if spatial_tolerance > 0 else labels

    # Flatten to 1D
    pred_flat = predictions.flatten().float()
    label_flat = labels.flatten().float()
    label_dilated_flat = labels_dilated.flatten().float()

    # Binary metrics with tolerance:
    # - TP: prediction=1 AND within tolerance of label=1
    # - TN: prediction=0 AND label=0
    # - FP: prediction=1 AND NOT within tolerance of any label=1
    # - FN: label=1 AND prediction=0 (or not within tolerance)

    # Use dilated labels for TP/FP calculation (spatial tolerance)
    tp = ((pred_flat == 1) & (label_dilated_flat == 1)).sum().item()
    fp = ((pred_flat == 1) & (label_dilated_flat == 0)).sum().item()

    # Use original labels for TN/FN (actual ground truth)
    tn = ((pred_flat == 0) & (label_flat == 0)).sum().item()
    fn = ((pred_flat == 0) & (label_flat == 1)).sum().item()

    # Calculate rates with smoothing to avoid division by zero
    eps = 1e-7
    sensitivity = tp / (tp + fn + eps)  # True positive rate
    specificity = tn / (tn + fp + eps)  # True negative rate
    informedness = sensitivity + specificity - 1.0

    # Also compute accuracy and IoU
    accuracy = (tp + tn) / (tp + tn + fp + fn + eps)
    iou = tp / (tp + fp + fn + eps)  # Intersection over union for satellite class

    return {
        'tp': tp,
        'tn': tn,
        'fp': fp,
        'fn': fn,
        'sensitivity': sensitivity,
        'specificity': specificity,
        'informedness': informedness,
        'accuracy': accuracy,
        'iou': iou
    }


def predict_sample_simple(
    model: SpikeSEGEncoder,
    x: torch.Tensor,
    device: torch.device,
    target_size: Tuple[int, int] = (128, 128)
) -> torch.Tensor:
    """
    Run inference on a single sample using simple interpolation.

    This is the fallback method that doesn't use HULK decoder.

    Args:
        model: Trained SpikeSEG encoder
        x: Input tensor (B, T, C, H, W) or (T, C, H, W)
        device: Target device
        target_size: Output size for upscaling

    Returns:
        Predicted class labels (B, H, W)
    """
    model.eval()

    with torch.no_grad():
        # Ensure correct shape
        if x.dim() == 4:
            x = x.unsqueeze(0)  # Add batch dim

        # Convert (B, T, C, H, W) -> (T, B, C, H, W) for encoder
        if x.dim() == 5:
            x = x.permute(1, 0, 2, 3, 4)

        x = x.to(device)

        # Forward pass
        output = model(x)

        # Get classification spikes: (T, B, C, H, W) -> (B, C, H, W)
        # Sum spikes over time
        class_spikes = output.classification_spikes.sum(dim=0)  # (B, C, H, W)

        # Predict class with most spikes at each spatial location
        # For binary: class 0 = background, class 1 = satellite
        predictions = class_spikes.argmax(dim=1)  # (B, H, W)

        # Upscale to target size
        if predictions.shape[-2:] != target_size:
            predictions = F.interpolate(
                predictions.unsqueeze(1).float(),
                size=target_size,
                mode='nearest'
            ).squeeze(1).long()

    return predictions


def predict_sample_hulk(
    model: SpikeSEGEncoder,
    hulk_decoder: HULKDecoder,
    x: torch.Tensor,
    device: torch.device,
    target_size: Tuple[int, int] = (128, 128),
    threshold: float = 0.5,
    invert_classes: bool = False
) -> Tuple[torch.Tensor, List[Instance]]:
    """
    Run inference using HULK decoder for proper spatial reconstruction.

    This implements the full IGARSS 2023 pipeline:
    1. Encoder produces classification spikes + pooling indices
    2. HULK unravels each spike back to pixel space
    3. Returns both pixel mask and instances for SMASH

    Args:
        model: Trained SpikeSEG encoder
        hulk_decoder: HULK decoder created from encoder
        x: Input tensor (B, T, C, H, W) or (T, C, H, W)
        device: Target device
        target_size: Output size (H, W)
        threshold: HULK threshold for contributing pixels
        invert_classes: If True, use class 0 as satellite instead of class 1

    Returns:
        Tuple of (predictions (B, H, W), instances list)
    """
    model.eval()

    with torch.no_grad():
        # Ensure correct shape
        if x.dim() == 4:
            x = x.unsqueeze(0)  # Add batch dim

        batch_size = x.shape[0]

        # Convert (B, T, C, H, W) -> (T, B, C, H, W) for encoder
        if x.dim() == 5:
            x = x.permute(1, 0, 2, 3, 4)

        n_timesteps = x.shape[0]
        x = x.to(device)

        # Forward pass
        output = model(x)

        # Get pooling indices
        pool_indices = output.pooling_indices

        # Initialize output mask
        H, W = target_size
        all_predictions = torch.zeros(batch_size, H, W, device=device)
        all_instances = []

        # Process each batch item
        for b in range(batch_size):
            # Get classification spikes for this batch item: (T, C, H, W)
            class_spikes = output.classification_spikes[:, b]

            # Check if any spikes occurred
            if class_spikes.sum() == 0:
                continue

            # Debug: Count spikes per class
            n_classes = class_spikes.shape[1]
            if n_classes == 1:
                # Detection mode: single class, all spikes are detections
                total_spikes = class_spikes.sum().item()
                if b == 0:
                    logger.debug(f"Detection mode: {total_spikes:.0f} total spikes")
                satellite_spikes = class_spikes  # Use all spikes
            else:
                # Classification mode: 2 classes (background/satellite)
                class0_mask = (class_spikes[:, 0, :, :].sum(dim=0) > 0)  # (H, W)
                class1_mask = (class_spikes[:, 1, :, :].sum(dim=0) > 0)  # (H, W)
                class0_spikes = class0_mask.sum().item()
                class1_spikes = class1_mask.sum().item()
                overlap = (class0_mask & class1_mask).sum().item()
                if b == 0:
                    logger.debug(f"Spikes: class0={class0_spikes:.0f}, class1={class1_spikes:.0f}, overlap={overlap:.0f}")

                # Create a filtered spike tensor with only satellite class
                satellite_spikes = class_spikes.clone()
                if invert_classes:
                    satellite_spikes[:, 1, :, :] = 0  # Zero out class 1 spikes
                else:
                    satellite_spikes[:, 0, :, :] = 0  # Zero out class 0 spikes

            # Use HULK to process satellite spikes to instances
            try:
                instances = hulk_decoder.process_to_instances(
                    classification_spikes=satellite_spikes,
                    pool1_indices=pool_indices.pool1_indices[b:b+1],
                    pool2_indices=pool_indices.pool2_indices[b:b+1],
                    pool1_output_size=pool_indices.pool1_output_size,
                    pool2_output_size=pool_indices.pool2_output_size,
                    n_timesteps=n_timesteps,
                    threshold=threshold
                )

                # Combine all instance masks into prediction
                for inst in instances:
                    if inst.mask is not None:
                        # Resize mask to target size if needed
                        mask = inst.mask
                        if mask.shape != (H, W):
                            mask = F.interpolate(
                                mask.unsqueeze(0).unsqueeze(0).float(),
                                size=(H, W),
                                mode='nearest'
                            ).squeeze()

                        # Mark satellite pixels (class 1)
                        all_predictions[b] = torch.maximum(
                            all_predictions[b],
                            mask.float()
                        )

                all_instances.extend(instances)

            except Exception as e:
                # Fallback to simple argmax if HULK fails
                logger.warning(f"HULK failed for batch {b}: {e}, using fallback")
                class_sum = class_spikes.sum(dim=0)  # (C, H, W)
                pred = class_sum.argmax(dim=0)  # (H, W)
                if pred.shape != (H, W):
                    pred = F.interpolate(
                        pred.unsqueeze(0).unsqueeze(0).float(),
                        size=(H, W),
                        mode='nearest'
                    ).squeeze()
                all_predictions[b] = pred.float()

    return all_predictions.long(), all_instances


def predict_sample(
    model: SpikeSEGEncoder,
    x: torch.Tensor,
    device: torch.device,
    hulk_decoder: Optional[HULKDecoder] = None,
    target_size: Tuple[int, int] = (128, 128),
    use_hulk: bool = True,
    invert_classes: bool = False
) -> Tuple[torch.Tensor, Optional[List[Instance]]]:
    """
    Run inference on a single sample.

    Args:
        model: Trained SpikeSEG encoder
        x: Input tensor (B, T, C, H, W) or (T, C, H, W)
        device: Target device
        hulk_decoder: Optional HULK decoder for proper spatial reconstruction
        target_size: Output size for predictions
        use_hulk: Whether to use HULK (if decoder provided)
        invert_classes: If True, use class 0 as satellite instead of class 1

    Returns:
        Tuple of (predictions (B, H, W), instances or None)
    """
    if use_hulk and hulk_decoder is not None:
        return predict_sample_hulk(model, hulk_decoder, x, device, target_size, invert_classes=invert_classes)
    else:
        predictions = predict_sample_simple(model, x, device, target_size)
        return predictions, None


def load_model(
    checkpoint_path: Path,
    device: torch.device,
    create_hulk: bool = True
) -> Tuple[SpikeSEGEncoder, Optional[HULKDecoder]]:
    """
    Load trained model from checkpoint and create HULK decoder.

    Args:
        checkpoint_path: Path to checkpoint .pt file
        device: Target device
        create_hulk: Whether to create HULK decoder

    Returns:
        Tuple of (SpikeSEG encoder, HULK decoder or None)
    """
    logger.info(f"Loading checkpoint: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    # Extract config from checkpoint or use IGARSS 2023 defaults
    # Default values match config.yaml and IGARSS 2023 paper
    default_thresholds = [0.1, 0.1, 0.1]
    default_leaks = [0.09, 0.01, 0.0]
    default_kernel_sizes = [5, 5, 7]
    default_conv1_channels = 4
    default_conv2_channels = 36
    default_pool1_kernel = 2
    default_pool1_stride = 2
    default_pool2_kernel = 2
    default_pool2_stride = 2

    # Get model parameters from checkpoint config
    try:
        model_cfg = checkpoint['config']['model']
        n_classes = model_cfg.get('n_classes', 1)
        thresholds = list(model_cfg.get('thresholds', default_thresholds))
        leaks = list(model_cfg.get('leaks', default_leaks))
        kernel_sizes = list(model_cfg.get('kernel_sizes', default_kernel_sizes))
        conv1_channels = model_cfg.get('conv1_channels', default_conv1_channels)
        conv2_channels = model_cfg.get('conv2_channels', default_conv2_channels)
        pool1_kernel = model_cfg.get('pool1_kernel', default_pool1_kernel)
        pool1_stride = model_cfg.get('pool1_stride', default_pool1_stride)
        pool2_kernel = model_cfg.get('pool2_kernel', default_pool2_kernel)
        pool2_stride = model_cfg.get('pool2_stride', default_pool2_stride)
        logger.info(f"Loaded model config from checkpoint: n_classes={n_classes}, "
                   f"thresholds={thresholds}, leaks={leaks}, "
                   f"channels=[{conv1_channels}, {conv2_channels}], kernels={kernel_sizes}")
    except (KeyError, TypeError):
        n_classes = 1  # Default to 1 (detection mode per IGARSS 2023)
        thresholds = default_thresholds
        leaks = default_leaks
        kernel_sizes = default_kernel_sizes
        conv1_channels = default_conv1_channels
        conv2_channels = default_conv2_channels
        pool1_kernel = default_pool1_kernel
        pool1_stride = default_pool1_stride
        pool2_kernel = default_pool2_kernel
        pool2_stride = default_pool2_stride
        logger.info(f"Using default model config: n_classes={n_classes}")

    # Get input_channels from checkpoint config
    try:
        input_channels = checkpoint['config']['data']['input_channels']
        logger.info(f"Loaded input_channels={input_channels} from checkpoint config")
    except (KeyError, TypeError):
        input_channels = 2  # Default to 2 (ON/OFF polarity)
        logger.info(f"Using default input_channels={input_channels}")

    enc_config = EncoderConfig(
        input_channels=input_channels,
        conv1=LayerConfig(out_channels=conv1_channels, kernel_size=kernel_sizes[0], threshold=thresholds[0], leak=leaks[0]),
        conv2=LayerConfig(out_channels=conv2_channels, kernel_size=kernel_sizes[1], threshold=thresholds[1], leak=leaks[1]),
        conv3=LayerConfig(out_channels=n_classes, kernel_size=kernel_sizes[2], threshold=thresholds[2], leak=leaks[2]),
        pool1_kernel_size=pool1_kernel,
        pool1_stride=pool1_stride,
        pool2_kernel_size=pool2_kernel,
        pool2_stride=pool2_stride,
    )

    model = SpikeSEGEncoder(enc_config)

    # Handle state dict with 'encoder.' prefix from training
    state_dict = checkpoint['model_state_dict']

    # Strip 'encoder.' prefix if present
    new_state_dict = {}
    for key, value in state_dict.items():
        if key.startswith('encoder.'):
            new_key = key[len('encoder.'):]
            new_state_dict[new_key] = value
        else:
            new_state_dict[key] = value

    # Filter out non-parameter keys (membrane, has_fired, etc.)
    param_keys = set(model.state_dict().keys())
    filtered_state_dict = {k: v for k, v in new_state_dict.items() if k in param_keys}

    model.load_state_dict(filtered_state_dict, strict=False)

    # Note: The model is already correctly initialized with leak values from the config.
    # LayerConfig.leak contains absolute leak values (e.g., 0.09 for 90% of threshold=0.1)
    # SpikingEncoderLayer converts to leak_factor = leak/threshold, then LIFNeuron
    # converts back to leak = leak_factor * threshold, so the neuron.leak buffer
    # is already set to the correct absolute leak value.
    # No need to overwrite - the model respects the checkpoint config.

    model.to(device)
    model.eval()

    logger.info(f"Model loaded: {enc_config.conv1.out_channels}→{enc_config.conv2.out_channels}→{enc_config.conv3.out_channels} features")

    # Create HULK decoder from encoder
    hulk_decoder = None
    if create_hulk:
        try:
            hulk_decoder = HULKDecoder.from_encoder(model)
            hulk_decoder.to(device)
            hulk_decoder.eval()
            logger.info(f"HULK decoder created: {hulk_decoder.n_features} features")
        except Exception as e:
            logger.warning(f"Failed to create HULK decoder: {e}")
            logger.warning("Falling back to simple interpolation")

    return model, hulk_decoder


def evaluate_volume_based(
    model: SpikeSEGEncoder,
    dataloader: DataLoader,
    device: torch.device,
    spatial_tolerance: float = 1.0,  # Paper: "one-pixel boundary" (IGARSS 2023)
    time_slice_ms: float = 10.0,
) -> Dict[str, float]:
    """
    Evaluate model using VOLUME-BASED event density (Afshar et al. 2019 / IGARSS 2023).

    This is the ORIGINAL paper methodology from Afshar et al.:
    "If, for any spatio-temporal volume, the event density is above the global
    event density of the full recording, then the volume is activated as positive."

    Algorithm:
    1. For each sample, compute global spike density (spikes/pixel/timestep)
    2. For each timestep, create spatio-temporal volume slices:
       - TRUE volume: within radius r of GT trajectory
       - FALSE volume: outside radius r
    3. For each volume slice:
       - POSITIVE if local density > global density
       - NEGATIVE if local density <= global density
    4. Metrics:
       - TP = TRUE & POSITIVE (detected where object is)
       - TN = FALSE & NEGATIVE (no detection where no object)
       - FP = FALSE & POSITIVE (detected where no object)
       - FN = TRUE & NEGATIVE (missed where object is)

    Args:
        model: Trained model
        dataloader: Test data loader
        device: Target device
        spatial_tolerance: Radius r for TRUE/FALSE volume boundary (default 1 pixel per paper)
        time_slice_ms: Time slice duration (default 10ms per Afshar paper)

    Returns:
        Dictionary with volume-based metrics
    """
    model.eval()

    # Accumulators for volume-based metrics
    total_tp_volumes = 0
    total_tn_volumes = 0
    total_fp_volumes = 0
    total_fn_volumes = 0
    n_samples = 0

    logger.info(f"Evaluating with VOLUME-BASED event density (Afshar et al. methodology)...")
    logger.info(f"  Spatial tolerance (radius r): {spatial_tolerance} pixel(s)")
    logger.info(f"  Time slice: {time_slice_ms}ms")

    for batch_idx, (x, labels) in enumerate(dataloader):
        labels = labels.to(device)

        # Ensure correct shape: (B, T, C, H, W)
        if x.dim() == 4:
            x = x.unsqueeze(0)
        batch_size = x.shape[0]

        # Convert (B, T, C, H, W) -> (T, B, C, H, W) for encoder
        if x.dim() == 5:
            x = x.permute(1, 0, 2, 3, 4)

        n_timesteps = x.shape[0]
        x = x.to(device)

        with torch.no_grad():
            output = model(x)

            for b in range(batch_size):
                # Get classification spikes: (T, C, H, W)
                class_spikes = output.classification_spikes[:, b]

                # Get GT label
                label_2d = labels[b] if labels.dim() == 3 else labels

                # Create distance map from GT trajectory
                # GT mask is 1 where satellite is, 0 elsewhere
                gt_mask = (label_2d > 0).cpu().numpy().astype(np.float32)

                if gt_mask.sum() == 0:
                    # No GT object - entire volume is FALSE
                    # Any spikes are FP, no spikes are TN
                    total_spikes = class_spikes.sum().item()
                    if total_spikes > 0:
                        total_fp_volumes += 1  # Detected in empty frame
                    else:
                        total_tn_volumes += 1  # Correctly no detection
                    n_samples += 1
                    continue

                # Compute distance transform from GT (distance to nearest GT pixel)
                from scipy.ndimage import distance_transform_edt
                distance_map = distance_transform_edt(1 - gt_mask)

                # Create TRUE/FALSE masks
                true_mask = distance_map <= spatial_tolerance  # Within r of GT
                false_mask = ~true_mask

                # Compute global spike density for this sample
                # Sum spikes over all timesteps and spatial locations
                spike_map = class_spikes.sum(dim=(0, 1)).cpu().numpy()  # (H, W)

                # Scale spike map to label size if needed
                if spike_map.shape != gt_mask.shape:
                    # Upscale spike map
                    scale_y = gt_mask.shape[0] / spike_map.shape[0]
                    scale_x = gt_mask.shape[1] / spike_map.shape[1]
                    from scipy.ndimage import zoom
                    spike_map = zoom(spike_map, (scale_y, scale_x), order=0)

                total_spikes = spike_map.sum()
                total_pixels = spike_map.size
                global_density = total_spikes / total_pixels if total_pixels > 0 else 0

                # Compute density in TRUE and FALSE regions
                true_spikes = spike_map[true_mask].sum() if true_mask.any() else 0
                true_pixels = true_mask.sum()
                true_density = true_spikes / true_pixels if true_pixels > 0 else 0

                false_spikes = spike_map[false_mask].sum() if false_mask.any() else 0
                false_pixels = false_mask.sum()
                false_density = false_spikes / false_pixels if false_pixels > 0 else 0

                # Volume-based classification:
                # TRUE region is POSITIVE if density > global mean
                # FALSE region is POSITIVE if density > global mean
                true_positive = true_density > global_density
                false_positive = false_density > global_density

                # Count volumes (each sample contributes 1 TRUE volume and 1 FALSE volume)
                if true_positive:
                    total_tp_volumes += 1  # Correctly detected object region
                else:
                    total_fn_volumes += 1  # Missed object region

                if false_positive:
                    total_fp_volumes += 1  # False alarm in background
                else:
                    total_tn_volumes += 1  # Correctly quiet background

                n_samples += 1

                # Debug output for first few samples
                if batch_idx < 3:
                    logger.info(f"  Sample {batch_idx}: global_density={global_density:.4f}, "
                               f"true_density={true_density:.4f}, false_density={false_density:.4f}")
                    logger.info(f"    TRUE region: {'POSITIVE (TP)' if true_positive else 'NEGATIVE (FN)'}")
                    logger.info(f"    FALSE region: {'POSITIVE (FP)' if false_positive else 'NEGATIVE (TN)'}")

    # Compute final metrics
    eps = 1e-7
    sensitivity = total_tp_volumes / (total_tp_volumes + total_fn_volumes + eps)
    specificity = total_tn_volumes / (total_tn_volumes + total_fp_volumes + eps)
    informedness = sensitivity + specificity - 1.0

    precision = total_tp_volumes / (total_tp_volumes + total_fp_volumes + eps)
    f1 = 2 * precision * sensitivity / (precision + sensitivity + eps)

    logger.info(f"")
    logger.info(f"  Volume counts: TP={total_tp_volumes}, TN={total_tn_volumes}, "
               f"FP={total_fp_volumes}, FN={total_fn_volumes}")

    return {
        'n_samples': n_samples,
        'total_tp': total_tp_volumes,
        'total_tn': total_tn_volumes,
        'total_fp': total_fp_volumes,
        'total_fn': total_fn_volumes,
        'sensitivity': sensitivity,
        'specificity': specificity,
        'informedness': informedness,
        'precision': precision,
        'f1': f1,
        'spatial_tolerance': spatial_tolerance
    }


def evaluate_objects(
    model: SpikeSEGEncoder,
    dataloader: DataLoader,
    device: torch.device,
    hulk_decoder: HULKDecoder,
    spatial_tolerance: float = 1.0,
    min_cluster_size: int = 1,
) -> Dict[str, float]:
    """
    Evaluate model using OBJECT-LEVEL centroid matching (paper methodology).

    Paper methodology (IGARSS 2023):
    "A correct prediction was determined if the centroid of the detection
    lands within 1 pixel of the centroid of the ground-truth"

    This evaluates object DETECTION, not pixel classification:
    - TP = detected object centroid within tolerance of GT object centroid
    - FP = detected object with no matching GT object
    - FN = GT object with no matching detection

    Args:
        model: Trained model
        dataloader: Test data loader
        device: Target device
        hulk_decoder: HULK decoder for spatial reconstruction
        spatial_tolerance: Max distance for centroid match (default 1 pixel)
        min_cluster_size: Minimum pixels in classification space for a detection (default 1)
                          Increase to filter out small noise clusters

    Returns:
        Dictionary with object-level metrics
    """
    model.eval()

    total_tp, total_fp, total_fn = 0, 0, 0
    total_detections = 0
    total_gt_objects = 0
    n_samples = 0

    # Diagnostic counters to understand FP sources
    samples_with_gt = 0
    samples_without_gt = 0
    detections_in_empty_frames = 0
    detections_in_gt_frames = 0

    logger.info(f"Evaluating with OBJECT-LEVEL trajectory matching...")
    logger.info(f"  Spatial tolerance: {spatial_tolerance} pixel(s)")
    logger.info(f"  Min cluster size: {min_cluster_size} pixel(s) in classification space")
    logger.info(f"  Mode: Detection must be within tolerance of ANY point on GT trajectory")

    for batch_idx, (x, labels) in enumerate(dataloader):
        labels = labels.to(device)

        # Ensure correct shape
        if x.dim() == 4:
            x = x.unsqueeze(0)
        batch_size = x.shape[0]

        # Convert (B, T, C, H, W) -> (T, B, C, H, W) for encoder
        if x.dim() == 5:
            x = x.permute(1, 0, 2, 3, 4)

        n_timesteps = x.shape[0]
        x = x.to(device)

        # Debug: check where input events are located
        if batch_idx < 3:
            # x is (T, B, C, H, W) - sum across time and channels
            input_frame = x.sum(dim=(0, 2))  # (B, H, W)
            for b_debug in range(min(batch_size, 1)):
                frame = input_frame[b_debug]  # (H, W)
                if frame.sum() > 0:
                    coords = torch.nonzero(frame > 0)
                    input_y = coords[:, 0].float().mean().item()
                    input_x = coords[:, 1].float().mean().item()
                    logger.info(f"    INPUT events center: ({input_x:.1f}, {input_y:.1f}), total pixels: {(frame > 0).sum().item()}")

        with torch.no_grad():
            output = model(x)
            pool_indices = output.pooling_indices

            for b in range(batch_size):
                # Get ALL classification spikes (both classes) for object detection
                class_spikes = output.classification_spikes[:, b]

                # Extract GT trajectory points (not just centroids!)
                # This fixes the bug where GT centroid was center of entire trail
                label_2d = labels[b] if labels.dim() == 3 else labels
                gt_points = extract_gt_centroids(label_2d, use_all_points=True)

                # Each sample has 1 GT object (satellite) even though it has many trajectory points
                has_gt = len(gt_points) > 0
                if has_gt:
                    total_gt_objects += 1
                    samples_with_gt += 1
                else:
                    samples_without_gt += 1

                if class_spikes.sum() == 0:
                    # No detections
                    if has_gt:
                        total_fn += 1  # Missed the object
                    if batch_idx < 3:
                        logger.info(f"  Sample {batch_idx}: detections=0, gt_points={len(gt_points)}")
                    n_samples += 1
                    continue

                # Debug: show classification spike locations
                if batch_idx < 3:
                    spike_locs = torch.nonzero(class_spikes.sum(dim=0))  # Sum over time
                    logger.info(f"    Classification map shape: {class_spikes.shape}")
                    logger.info(f"    Total classification spikes: {class_spikes.sum().item():.0f}")
                    if len(spike_locs) > 0:
                        logger.info(f"    Spike locations (class, y, x) first 10: {spike_locs[:10].tolist()}")
                    # Check label shape and GT info
                    logger.info(f"    Label shape: {label_2d.shape}, GT trajectory points: {len(gt_points)}")
                    if gt_points:
                        logger.info(f"    GT trajectory span: x=[{min(p[0] for p in gt_points):.0f}, {max(p[0] for p in gt_points):.0f}], "
                                   f"y=[{min(p[1] for p in gt_points):.0f}, {max(p[1] for p in gt_points):.0f}]")

                # WORKAROUND: HULK unpooling is broken because pooling indices are only
                # from the last timestep, but spikes occur across all timesteps.
                # Use classification spike locations directly (scaled by 4) instead.
                spike_locs = torch.nonzero(class_spikes.sum(dim=0))  # (N, 3) for [class, y, x]
                if len(spike_locs) > 0:
                    # Scale from classification map to pixel space
                    # Input 128x128 -> Pool1 64x64 -> Pool2 32x32
                    # So scale factor = 128/32 = 4
                    class_h, class_w = class_spikes.shape[2], class_spikes.shape[3]
                    target_h, target_w = label_2d.shape[0], label_2d.shape[1]
                    scale_factor_y = target_h / class_h
                    scale_factor_x = target_w / class_w

                    # Use connected components for proper clustering (not naive grid)
                    # Create binary mask from spike locations in classification space
                    spike_mask = (class_spikes.sum(dim=0).sum(dim=0) > 0).cpu().numpy()

                    # Find connected components
                    labeled_array, num_features = ndimage.label(spike_mask)

                    # Extract centroid for each connected component (filter by min size)
                    det_centroids = []
                    for i in range(1, num_features + 1):
                        coords = np.where(labeled_array == i)
                        cluster_size = len(coords[0])
                        if cluster_size >= min_cluster_size:
                            # Centroid in classification space, then scale to pixel space
                            cy = coords[0].mean() * scale_factor_y
                            cx = coords[1].mean() * scale_factor_x
                            det_centroids.append((cx, cy))  # (x, y) format

                    total_detections += len(det_centroids)
                    # Track where detections are occurring
                    if has_gt:
                        detections_in_gt_frames += len(det_centroids)
                    else:
                        detections_in_empty_frames += len(det_centroids)
                else:
                    det_centroids = []

                # Compute object-level metrics using trajectory matching
                # trajectory_mode=True: detection near ANY trajectory point = 1 TP
                metrics = compute_object_metrics(
                    det_centroids, gt_points, spatial_tolerance, trajectory_mode=True
                )
                total_tp += metrics['tp']
                total_fp += metrics['fp']
                total_fn += metrics['fn']

                if batch_idx < 3:
                    logger.info(f"  Sample {batch_idx}: detections={len(det_centroids)}, "
                               f"gt_points={len(gt_points)}, tp={metrics['tp']}, fp={metrics['fp']}")
                    # Debug: show actual centroid coordinates
                    if det_centroids and gt_points:
                        logger.info(f"    Det centroids: {det_centroids[:3]}")
                        # Find minimum distance from any detection to trajectory
                        min_dist = float('inf')
                        for dx, dy in det_centroids:
                            for gx, gy in gt_points:
                                dist = np.sqrt((dx - gx)**2 + (dy - gy)**2)
                                min_dist = min(min_dist, dist)
                        logger.info(f"    Min distance from detection to trajectory: {min_dist:.1f} pixels")

        n_samples += batch_size

    # Compute final metrics
    eps = 1e-7
    # For object detection: Sensitivity = TP / (TP + FN), Specificity needs TN
    # Since we don't have TN for object detection, compute precision/recall instead
    sensitivity = total_tp / (total_tp + total_fn + eps)  # Recall
    precision = total_tp / (total_tp + total_fp + eps)

    # For informedness, we need specificity. In object detection context:
    # Specificity = TN / (TN + FP), but TN is not well-defined for object detection
    # The paper likely computes it per-frame: specificity = 1 - FP_rate
    # where FP_rate = FP / total_detections
    if total_detections > 0:
        specificity = 1.0 - (total_fp / total_detections)
    else:
        specificity = 1.0

    informedness = sensitivity + specificity - 1.0
    f1 = 2 * precision * sensitivity / (precision + sensitivity + eps)

    logger.info(f"  Total detections: {total_detections}")
    logger.info(f"  Total GT objects: {total_gt_objects}")
    logger.info(f"  Matched (TP): {total_tp}")

    # Diagnostic: where are FPs coming from?
    logger.info(f"")
    logger.info(f"  --- FP Source Analysis ---")
    logger.info(f"  Samples with GT (satellites): {samples_with_gt}")
    logger.info(f"  Samples without GT (empty): {samples_without_gt}")
    logger.info(f"  Detections in GT frames: {detections_in_gt_frames} (TP={total_tp}, FP in GT frames={detections_in_gt_frames - total_tp})")
    logger.info(f"  Detections in empty frames: {detections_in_empty_frames} (all FP)")

    return {
        'n_samples': n_samples,
        'total_tp': total_tp,
        'total_fp': total_fp,
        'total_fn': total_fn,
        'total_detections': total_detections,
        'total_gt_objects': total_gt_objects,
        'sensitivity': sensitivity,
        'specificity': specificity,
        'informedness': informedness,
        'precision': precision,
        'f1': f1,
        'spatial_tolerance': spatial_tolerance
    }


def evaluate(
    model: SpikeSEGEncoder,
    dataloader: DataLoader,
    device: torch.device,
    hulk_decoder: Optional[HULKDecoder] = None,
    spatial_tolerance: int = 1,
    use_hulk: bool = True,
    use_smash: bool = True,
    invert_classes: bool = False
) -> Dict[str, float]:
    """
    Evaluate model on dataset using PIXEL-LEVEL metrics (legacy).

    For paper-matching evaluation, use evaluate_objects() instead.

    Args:
        model: Trained model
        dataloader: Test data loader
        device: Target device
        hulk_decoder: HULK decoder for proper spatial reconstruction
        spatial_tolerance: Pixels of tolerance for TP (default 1 per paper)
        use_hulk: Whether to use HULK decoder
        use_smash: Whether to use SMASH grouping
        invert_classes: If True, use class 0 as satellite instead of class 1

    Returns:
        Aggregated metrics
    """
    # Accumulators
    total_tp, total_tn, total_fp, total_fn = 0, 0, 0, 0
    n_samples = 0

    logger.info(f"Evaluating with PIXEL-LEVEL metrics...")
    logger.info(f"  Using HULK decoder: {use_hulk and hulk_decoder is not None}")
    logger.info(f"  Spatial tolerance: {spatial_tolerance} pixel(s)")
    logger.info(f"  Using SMASH grouping: {use_smash}")
    logger.info(f"  Invert classes: {invert_classes} (satellite=class {'0' if invert_classes else '1'})")

    total_spikes = 0
    total_label_positives = 0
    total_instances = 0
    total_objects = 0

    for batch_idx, (x, labels) in enumerate(dataloader):
        # Move labels to device
        labels = labels.to(device)
        target_size = labels.shape[-2:]

        # Get predictions using HULK if available
        predictions, instances = predict_sample(
            model, x, device,
            hulk_decoder=hulk_decoder,
            target_size=target_size,
            use_hulk=use_hulk,
            invert_classes=invert_classes
        )

        # Debug: count spikes and positive labels
        total_spikes += (predictions == 1).sum().item()
        total_label_positives += (labels == 1).sum().item()

        # Track instances and objects if using HULK
        if instances:
            total_instances += len(instances)
            if use_smash:
                objects = group_instances_to_objects(instances, smash_threshold=0.1)
                total_objects += len(objects)

        # Debug output for first few samples
        if batch_idx < 3:
            n_inst = len(instances) if instances else 0
            logger.info(f"  Sample {batch_idx}: pred_sum={predictions.sum().item()}, "
                       f"label_sum={labels.sum().item()}, instances={n_inst}")

        # Ensure predictions match label size
        if predictions.shape[-2:] != labels.shape[-2:]:
            predictions = F.interpolate(
                predictions.unsqueeze(1).float(),
                size=labels.shape[-2:],
                mode='nearest'
            ).squeeze(1).long()

        # Compute metrics for this batch with spatial tolerance
        metrics = compute_metrics(predictions, labels, spatial_tolerance=spatial_tolerance)

        total_tp += metrics['tp']
        total_tn += metrics['tn']
        total_fp += metrics['fp']
        total_fn += metrics['fn']
        n_samples += x.shape[0] if x.dim() == 5 else 1

        if (batch_idx + 1) % 100 == 0:
            logger.info(f"  [{batch_idx + 1}/{len(dataloader)}] samples processed")

    logger.info(f"  Total predicted positives: {total_spikes}")
    logger.info(f"  Total label positives: {total_label_positives}")
    if total_instances > 0:
        logger.info(f"  Total instances (HULK): {total_instances}")
        if use_smash:
            logger.info(f"  Total objects (SMASH): {total_objects}")

    # Final aggregated metrics
    eps = 1e-7
    sensitivity = total_tp / (total_tp + total_fn + eps)
    specificity = total_tn / (total_tn + total_fp + eps)
    informedness = sensitivity + specificity - 1.0
    accuracy = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn + eps)
    iou = total_tp / (total_tp + total_fp + total_fn + eps)

    return {
        'n_samples': n_samples,
        'total_tp': total_tp,
        'total_tn': total_tn,
        'total_fp': total_fp,
        'total_fn': total_fn,
        'sensitivity': sensitivity,
        'specificity': specificity,
        'informedness': informedness,
        'accuracy': accuracy,
        'iou': iou,
        'total_instances': total_instances,
        'total_objects': total_objects,
        'spatial_tolerance': spatial_tolerance
    }


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate SpikeSEG model with HULK-SMASH pipeline"
    )
    parser.add_argument(
        '--checkpoint', '-c',
        type=str,
        required=True,
        help='Path to model checkpoint'
    )
    parser.add_argument(
        '--data-root', '-d',
        type=str,
        default='../ebssa-data-utah/ebssa',
        help='Path to EBSSA dataset'
    )
    parser.add_argument(
        '--split',
        type=str,
        default='all',
        choices=['train', 'val', 'test', 'all'],
        help='Dataset split to evaluate'
    )
    parser.add_argument(
        '--sensor',
        type=str,
        default='all',
        choices=['ATIS', 'DAVIS', 'all'],
        help='Sensor type to evaluate'
    )
    parser.add_argument(
        '--batch-size', '-b',
        type=int,
        default=1,
        help='Batch size'
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cuda' if torch.cuda.is_available() else 'cpu',
        help='Device to use'
    )
    parser.add_argument(
        '--output', '-o',
        type=str,
        default=None,
        help='Output JSON file for metrics (optional)'
    )
    parser.add_argument(
        '--spatial-tolerance',
        type=int,
        default=1,
        help='Spatial tolerance in pixels for TP (default: 1 per IGARSS 2023 paper "one-pixel boundary")'
    )
    parser.add_argument(
        '--no-hulk',
        action='store_true',
        help='Disable HULK decoder (use simple interpolation instead)'
    )
    parser.add_argument(
        '--no-smash',
        action='store_true',
        help='Disable SMASH instance grouping'
    )
    parser.add_argument(
        '--invert-classes',
        action='store_true',
        help='Invert class assignment (use class 0 as satellite instead of class 1)'
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help='Enable debug logging'
    )
    parser.add_argument(
        '--object-level',
        action='store_true',
        help='Use object-level centroid matching (paper methodology) instead of pixel-level'
    )
    parser.add_argument(
        '--min-cluster-size',
        type=int,
        default=1,
        help='Minimum cluster size in classification space to count as detection (default: 1). '
             'Increase to filter out small noise detections (e.g., 3-5).'
    )
    parser.add_argument(
        '--volume-based',
        action='store_true',
        help='Use volume-based event density evaluation (Afshar et al. 2019 / IGARSS 2023 methodology). '
             'This is the ORIGINAL paper methodology that achieved 89.1%% informedness.'
    )

    args = parser.parse_args()

    # Set log level
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    device = torch.device(args.device)
    logger.info(f"Using device: {device}")

    # Load model and HULK decoder
    checkpoint_path = Path(args.checkpoint)
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

    model, hulk_decoder = load_model(
        checkpoint_path, device,
        create_hulk=not args.no_hulk
    )

    # Load dataset
    logger.info(f"Loading dataset from: {args.data_root}")
    dataset = EBSSADataset(
        root=args.data_root,
        split=args.split,
        sensor=args.sensor,
        n_timesteps=20,
        height=128,
        width=128,
        normalize=True,
        use_labels=True,
        windows_per_recording=1,  # Use object-time filtering (same as training)
    )

    logger.info(f"Dataset: {len(dataset)} samples")

    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0,  # Avoid OOM with spatial filtering
        pin_memory=True
    )

    # Evaluate
    logger.info("=" * 60)
    if args.volume_based:
        logger.info("  SpikeSEG VOLUME-BASED Evaluation (Afshar/IGARSS 2023 Methodology)")
    elif args.object_level:
        logger.info("  SpikeSEG OBJECT-LEVEL Evaluation (Centroid Matching)")
    else:
        logger.info("  SpikeSEG Pixel-Level Evaluation with HULK-SMASH")
    logger.info("=" * 60)

    if args.volume_based:
        # Volume-based evaluation (original paper methodology)
        metrics = evaluate_volume_based(
            model, dataloader, device,
            spatial_tolerance=float(args.spatial_tolerance)
        )

        # Print volume-based results
        logger.info("")
        logger.info("=" * 60)
        logger.info("  VOLUME-BASED Results (Afshar et al. / IGARSS 2023)")
        logger.info("=" * 60)
        logger.info(f"  Samples:        {metrics['n_samples']}")
        logger.info(f"  TP/TN/FP/FN:    {metrics['total_tp']}/{metrics['total_tn']}/{metrics['total_fp']}/{metrics['total_fn']}")
        logger.info("")
        logger.info(f"  Sensitivity:    {metrics['sensitivity']*100:.1f}%")
        logger.info(f"  Specificity:    {metrics['specificity']*100:.1f}%")
        logger.info(f"  Informedness:   {metrics['informedness']*100:.1f}%  (paper: 89.1%)")
        logger.info(f"  Precision:      {metrics['precision']*100:.1f}%")
        logger.info(f"  F1 Score:       {metrics['f1']*100:.1f}%")
        logger.info("=" * 60)

    elif args.object_level:
        if hulk_decoder is None:
            logger.error("Object-level evaluation requires HULK decoder. Don't use --no-hulk.")
            return None

        metrics = evaluate_objects(
            model, dataloader, device,
            hulk_decoder=hulk_decoder,
            spatial_tolerance=float(args.spatial_tolerance),
            min_cluster_size=args.min_cluster_size
        )

        # Print object-level results
        logger.info("")
        logger.info("=" * 60)
        logger.info("  OBJECT-LEVEL Results (Paper Methodology)")
        logger.info("=" * 60)
        logger.info(f"  Samples:        {metrics['n_samples']}")
        logger.info(f"  GT Objects:     {metrics['total_gt_objects']}")
        logger.info(f"  Detections:     {metrics['total_detections']}")
        logger.info(f"  TP/FP/FN:       {metrics['total_tp']}/{metrics['total_fp']}/{metrics['total_fn']}")
        logger.info("")
        logger.info(f"  Sensitivity:    {metrics['sensitivity']*100:.1f}%  (Recall)")
        logger.info(f"  Specificity:    {metrics['specificity']*100:.1f}%")
        logger.info(f"  Precision:      {metrics['precision']*100:.1f}%")
        logger.info(f"  F1 Score:       {metrics['f1']*100:.1f}%")
        logger.info(f"  Informedness:   {metrics['informedness']*100:.1f}%  (paper: 89.1%)")
        logger.info("=" * 60)
    else:
        metrics = evaluate(
            model, dataloader, device,
            hulk_decoder=hulk_decoder,
            spatial_tolerance=args.spatial_tolerance,
            use_hulk=not args.no_hulk,
            use_smash=not args.no_smash,
            invert_classes=args.invert_classes
        )

        # Print pixel-level results
        logger.info("")
        logger.info("=" * 60)
        logger.info("  PIXEL-LEVEL Results")
        logger.info("=" * 60)
        logger.info(f"  Samples:       {metrics['n_samples']}")
        logger.info(f"  TP/TN/FP/FN:   {metrics['total_tp']}/{metrics['total_tn']}/{metrics['total_fp']}/{metrics['total_fn']}")
        if metrics.get('total_instances', 0) > 0:
            logger.info(f"  Instances:     {metrics['total_instances']}")
            logger.info(f"  Objects:       {metrics['total_objects']}")
        logger.info("")
        logger.info(f"  Sensitivity:   {metrics['sensitivity']*100:.1f}%")
        logger.info(f"  Specificity:   {metrics['specificity']*100:.1f}%")
        logger.info(f"  Informedness:  {metrics['informedness']*100:.1f}%  (paper: 89.1%)")
        logger.info(f"  Accuracy:      {metrics['accuracy']*100:.1f}%")
        logger.info(f"  IoU:           {metrics['iou']*100:.1f}%")
        logger.info("=" * 60)

    # Save to JSON if requested
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        logger.info(f"Metrics saved: {output_path}")

    return metrics


if __name__ == '__main__':
    main()
