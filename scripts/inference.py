#!/usr/bin/env python3
"""
SpikeSEG Inference Script - Satellite Detection with Bounding Boxes

Runs inference on event camera data and outputs detected satellite bounding boxes.

Usage:
    python scripts/inference.py --checkpoint runs/.../checkpoint_best.pt --input data.mat
    python scripts/inference.py --checkpoint ... --data-root ../ebssa-data-utah/ebssa --visualize
"""

import argparse
import json
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple

import numpy as np
import torch
import torch.nn.functional as F
from scipy import ndimage

from spikeseg.models.encoder import SpikeSEGEncoder, EncoderConfig, LayerConfig


@dataclass
class BoundingBox:
    """Detected satellite bounding box."""
    x_min: float
    y_min: float
    x_max: float
    y_max: float
    confidence: float = 1.0

    @property
    def center(self) -> Tuple[float, float]:
        return ((self.x_min + self.x_max) / 2, (self.y_min + self.y_max) / 2)

    @property
    def width(self) -> float:
        return self.x_max - self.x_min

    @property
    def height(self) -> float:
        return self.y_max - self.y_min

    def to_dict(self) -> dict:
        return {
            'x_min': self.x_min,
            'y_min': self.y_min,
            'x_max': self.x_max,
            'y_max': self.y_max,
            'center_x': self.center[0],
            'center_y': self.center[1],
            'width': self.width,
            'height': self.height,
            'confidence': self.confidence
        }


def merge_overlapping_boxes(boxes: List[BoundingBox], iou_threshold: float = 0.3) -> List[BoundingBox]:
    """
    Merge overlapping bounding boxes into single consolidated boxes.

    Args:
        boxes: List of detected bounding boxes
        iou_threshold: IoU threshold for merging (lower = more aggressive merging)

    Returns:
        List of merged bounding boxes
    """
    if len(boxes) <= 1:
        return boxes

    # Convert to numpy for easier manipulation
    coords = np.array([[b.x_min, b.y_min, b.x_max, b.y_max, b.confidence] for b in boxes])

    # Iteratively merge overlapping boxes
    merged = True
    while merged:
        merged = False
        n = len(coords)
        if n <= 1:
            break

        # Find pairs to merge
        to_merge = []
        used = set()

        for i in range(n):
            if i in used:
                continue
            for j in range(i + 1, n):
                if j in used:
                    continue

                # Calculate IoU
                x1 = max(coords[i, 0], coords[j, 0])
                y1 = max(coords[i, 1], coords[j, 1])
                x2 = min(coords[i, 2], coords[j, 2])
                y2 = min(coords[i, 3], coords[j, 3])

                if x2 > x1 and y2 > y1:
                    intersection = (x2 - x1) * (y2 - y1)
                    area_i = (coords[i, 2] - coords[i, 0]) * (coords[i, 3] - coords[i, 1])
                    area_j = (coords[j, 2] - coords[j, 0]) * (coords[j, 3] - coords[j, 1])
                    union = area_i + area_j - intersection
                    iou = intersection / union if union > 0 else 0

                    # Also check if boxes are close (within 10 pixels)
                    close = (abs(coords[i, 0] - coords[j, 2]) < 10 or
                             abs(coords[j, 0] - coords[i, 2]) < 10 or
                             abs(coords[i, 1] - coords[j, 3]) < 10 or
                             abs(coords[j, 1] - coords[i, 3]) < 10)

                    if iou > iou_threshold or (iou > 0 and close):
                        to_merge.append((i, j))
                        used.add(i)
                        used.add(j)
                        merged = True
                        break
            if merged:
                break

        # Merge found pairs
        if to_merge:
            new_coords = []
            merged_indices = set()

            for i, j in to_merge:
                # Merge boxes i and j
                new_box = [
                    min(coords[i, 0], coords[j, 0]),  # x_min
                    min(coords[i, 1], coords[j, 1]),  # y_min
                    max(coords[i, 2], coords[j, 2]),  # x_max
                    max(coords[i, 3], coords[j, 3]),  # y_max
                    max(coords[i, 4], coords[j, 4]),  # confidence
                ]
                new_coords.append(new_box)
                merged_indices.add(i)
                merged_indices.add(j)

            # Keep unmerged boxes
            for i in range(n):
                if i not in merged_indices:
                    new_coords.append(coords[i].tolist())

            coords = np.array(new_coords)

    # Convert back to BoundingBox objects
    return [
        BoundingBox(
            x_min=float(c[0]),
            y_min=float(c[1]),
            x_max=float(c[2]),
            y_max=float(c[3]),
            confidence=float(c[4])
        )
        for c in coords
    ]


def load_model(checkpoint_path: str, device: torch.device, inference_threshold: float = 0.05):
    """Load model from checkpoint with optimal inference threshold."""
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    # Extract config
    config = checkpoint.get('config', {})
    model_cfg = config.get('model', {})

    n_classes = model_cfg.get('n_classes', 1)
    kernel_sizes = model_cfg.get('kernel_sizes', [5, 5, 7])
    conv1_channels = model_cfg.get('conv1_channels', 4)
    conv2_channels = model_cfg.get('conv2_channels', 36)
    input_channels = config.get('data', {}).get('input_channels', 2)

    # Use inference threshold (0.05 works best based on evaluation)
    thresholds = [inference_threshold] * 3
    leak_ratios = [0.9, 0.1, 0.0]
    leaks = [inference_threshold * r for r in leak_ratios]

    enc_config = EncoderConfig(
        input_channels=input_channels,
        conv1=LayerConfig(out_channels=conv1_channels, kernel_size=kernel_sizes[0],
                         threshold=thresholds[0], leak=leaks[0]),
        conv2=LayerConfig(out_channels=conv2_channels, kernel_size=kernel_sizes[1],
                         threshold=thresholds[1], leak=leaks[1]),
        conv3=LayerConfig(out_channels=n_classes, kernel_size=kernel_sizes[2],
                         threshold=thresholds[2], leak=leaks[2]),
    )

    model = SpikeSEGEncoder(enc_config)

    # Load weights
    state_dict = checkpoint['model_state_dict']
    new_state_dict = {}
    for key, value in state_dict.items():
        if key.startswith('encoder.'):
            new_state_dict[key[len('encoder.'):]] = value
        else:
            new_state_dict[key] = value

    param_keys = set(model.state_dict().keys())
    filtered_state_dict = {k: v for k, v in new_state_dict.items() if k in param_keys}
    model.load_state_dict(filtered_state_dict, strict=False)

    # Set inference thresholds AFTER load_state_dict
    for i, (layer_name, leak_val) in enumerate([
        ('conv1', leaks[0]), ('conv2', leaks[1]), ('conv3', leaks[2])
    ]):
        layer = getattr(model, layer_name)
        if hasattr(layer, 'neuron'):
            layer.neuron.threshold.fill_(inference_threshold)
            layer.neuron.leak.fill_(leak_val)

    model.to(device)
    model.eval()

    return model


def detect_satellites(
    model: SpikeSEGEncoder,
    events: torch.Tensor,
    device: torch.device,
    min_cluster_pixels: int = 2,
    return_spikes: bool = False,
    return_pooling_indices: bool = False,
    inference_mode: bool = False,
) -> List[BoundingBox]:
    """
    Run inference and return detected satellite bounding boxes.

    Args:
        model: Loaded SpikeSEG model
        events: Input tensor (T, C, H, W) or (B, T, C, H, W)
        device: Compute device
        min_cluster_pixels: Minimum cluster size to count as detection
        return_spikes: If True, also return raw classification spikes for visualization
        return_pooling_indices: If True, also return pooling indices for HULK decoder
        inference_mode: If True, disable fire-once constraint for continuous tracking.
                       Neurons can fire multiple times as satellite moves (IGARSS 2023).

    Returns:
        List of BoundingBox for detected satellites
        If return_spikes=True: (boxes, classification_spikes)
        If return_pooling_indices=True: (boxes, classification_spikes, pooling_indices)
    """
    model.eval()

    with torch.no_grad():
        # Ensure correct shape: (T, B, C, H, W)
        if events.dim() == 4:
            events = events.unsqueeze(1)  # Add batch dim -> (T, 1, C, H, W)
        elif events.dim() == 5 and events.shape[0] < events.shape[1]:
            # Input is (B, T, C, H, W) where B < T, convert to (T, B, C, H, W)
            # Note: Only permute when first dim < second dim (batch typically smaller than timesteps)
            events = events.permute(1, 0, 2, 3, 4)

        events = events.to(device)
        input_h, input_w = events.shape[-2], events.shape[-1]

        # Forward pass
        # inference_mode=True disables fire-once for continuous tracking
        output = model(events, fire_once=not inference_mode)

        # Store raw spikes for 3D visualization (T, B, C, H, W)
        raw_spikes = output.classification_spikes.clone()

        # Store pooling indices for HULK decoder
        pooling_indices = output.pooling_indices if return_pooling_indices else None

        # Get classification spikes: (T, B, C, H, W) -> sum over time
        class_spikes = output.classification_spikes.sum(dim=0)  # (B, C, H, W)

        # Process each batch item
        all_boxes = []
        for b in range(class_spikes.shape[0]):
            # Sum over channels to get spike map
            spike_map = class_spikes[b].sum(dim=0).cpu().numpy()  # (H, W)

            if spike_map.sum() == 0:
                continue

            # Scale factor from classification space to input space
            class_h, class_w = spike_map.shape
            scale_y = input_h / class_h
            scale_x = input_w / class_w

            # Offset to account for receptive field of convolutions
            # Conv1(k=5) + Conv2(k=5) + Conv3(k=7) = total ~12 pixel offset
            offset = 12

            # Find connected components
            binary_map = (spike_map > 0).astype(np.uint8)
            labeled_array, num_features = ndimage.label(binary_map)

            # Extract bounding box for each component
            for i in range(1, num_features + 1):
                coords = np.where(labeled_array == i)
                if len(coords[0]) < min_cluster_pixels:
                    continue

                # Bounding box in classification space
                y_min_class = coords[0].min()
                y_max_class = coords[0].max()
                x_min_class = coords[1].min()
                x_max_class = coords[1].max()

                # Scale to input space and add offset
                x_min = x_min_class * scale_x + offset
                x_max = (x_max_class + 1) * scale_x + offset
                y_min = y_min_class * scale_y + offset
                y_max = (y_max_class + 1) * scale_y + offset

                # Confidence based on spike density
                cluster_spikes = spike_map[coords].sum()
                confidence = min(1.0, cluster_spikes / 10.0)

                all_boxes.append(BoundingBox(
                    x_min=float(x_min),
                    y_min=float(y_min),
                    x_max=float(x_max),
                    y_max=float(y_max),
                    confidence=float(confidence)
                ))

        if return_pooling_indices:
            return all_boxes, raw_spikes, pooling_indices
        if return_spikes:
            return all_boxes, raw_spikes
        return all_boxes


def run_hulk_smash_tracking(
    model: SpikeSEGEncoder,
    events: torch.Tensor,
    device: torch.device,
    previous_objects: Optional[list] = None,
    smash_threshold: float = 0.0,
    inference_mode: bool = False,
):
    """
    Run HULK/SMASH instance segmentation and tracking.

    This implements the full pipeline from the Kirkland et al. 2022 paper:
    1. HULK: Decode each classification spike back to pixel space
    2. ASH: Create Active Spike Hash for each instance
    3. SMASH: Group instances into objects using SMASH scores
    4. Track: Match objects across sequences

    Args:
        model: Loaded SpikeSEG encoder
        events: Input tensor (T, C, H, W) or (T, B, C, H, W)
        device: Compute device
        previous_objects: Objects from previous sequence for tracking (optional)
        smash_threshold: Minimum SMASH score to group instances (default 0.0)
        inference_mode: If True, disable fire-once for continuous tracking
        smash_threshold: Minimum SMASH score to group instances (default: 0.0)

    Returns:
        Dict with:
            - 'objects': List of Object instances detected
            - 'instances': List of Instance instances (before grouping)
            - 'matches': Dict mapping current object IDs to previous object IDs
            - 'n_spikes': Total number of classification spikes processed
    """
    from spikeseg.algorithms.hulk import HULKDecoder
    from spikeseg.algorithms.smash import group_instances_to_objects, match_objects_across_sequences

    model.eval()

    with torch.no_grad():
        # Ensure correct shape: (T, B, C, H, W)
        if events.dim() == 4:
            events = events.unsqueeze(1)
        elif events.dim() == 5 and events.shape[0] < events.shape[1]:
            # Input is (B, T, C, H, W) where B < T, convert to (T, B, C, H, W)
            events = events.permute(1, 0, 2, 3, 4)

        events = events.to(device)
        T = events.shape[0]

        # Forward pass (inference_mode disables fire-once for continuous tracking)
        output = model(events, fire_once=not inference_mode)

        # Get classification spikes and pooling indices
        class_spikes = output.classification_spikes  # (T, B, C, H, W)
        pool_indices = output.pooling_indices

        # Create HULK decoder from encoder weights
        hulk = HULKDecoder.from_encoder(model)

        # Process batch item 0 (single sample)
        # Reshape class_spikes: (T, B, C, H, W) -> (T, C, H, W) for batch 0
        class_spikes_b0 = class_spikes[:, 0, :, :, :]  # (T, C, H, W)

        # Count total spikes
        n_spikes = int(class_spikes_b0.sum().item())

        if n_spikes == 0:
            return {
                'objects': [],
                'instances': [],
                'matches': {},
                'n_spikes': 0
            }

        # Get pooling indices for batch 0
        # HULK expects batch size = 1, so slice if needed
        pool1_idx = pool_indices.pool1_indices
        pool2_idx = pool_indices.pool2_indices
        pool1_size = pool_indices.pool1_output_size
        pool2_size = pool_indices.pool2_output_size

        # Ensure pooling indices have batch size = 1
        if pool1_idx.shape[0] > 1:
            pool1_idx = pool1_idx[0:1]
        if pool2_idx.shape[0] > 1:
            pool2_idx = pool2_idx[0:1]

        # Use HULK to process all classification spikes into instances
        try:
            instances = hulk.process_to_instances(
                classification_spikes=class_spikes_b0,
                pool1_indices=pool1_idx,
                pool2_indices=pool2_idx,
                pool1_output_size=pool1_size,
                pool2_output_size=pool2_size,
                n_timesteps=T,
                threshold=0.5
            )
        except Exception as e:
            print(f"HULK processing failed: {e}")
            return {
                'objects': [],
                'instances': [],
                'matches': {},
                'n_spikes': n_spikes,
                'error': str(e)
            }

        # Group instances into objects using SMASH
        objects = group_instances_to_objects(instances, smash_threshold=smash_threshold)

        # Match with previous objects if provided
        matches = {}
        if previous_objects and objects:
            try:
                matches = match_objects_across_sequences(
                    objects, previous_objects, similarity_threshold=0.1
                )
            except ValueError as e:
                # ASH dimension mismatch (different n_timesteps between sequences)
                print(f"  Warning: Cross-sequence matching failed: {e}")
                matches = {}

        return {
            'objects': objects,
            'instances': instances,
            'matches': matches,
            'n_spikes': n_spikes
        }


def visualize_detections(
    events: torch.Tensor,
    boxes: List[BoundingBox],
    label: Optional[torch.Tensor] = None,
    output_path: Optional[str] = None
):
    """Visualize detections on event frame."""
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches

    # Sum events over time and channels
    if events.dim() == 5:
        event_frame = events.sum(dim=(0, 1, 2)).cpu().numpy()
    elif events.dim() == 4:
        event_frame = events.sum(dim=(0, 1)).cpu().numpy()
    else:
        event_frame = events.cpu().numpy()

    fig, ax = plt.subplots(1, 1, figsize=(10, 10))
    ax.imshow(event_frame, cmap='gray')

    # Draw detected boxes
    for box in boxes:
        rect = patches.Rectangle(
            (box.x_min, box.y_min),
            box.width, box.height,
            linewidth=2, edgecolor='lime', facecolor='none'
        )
        ax.add_patch(rect)
        ax.plot(*box.center, 'g+', markersize=10, markeredgewidth=2)

    # Draw ground truth if available
    if label is not None:
        label_np = label.cpu().numpy() if isinstance(label, torch.Tensor) else label
        if label_np.sum() > 0:
            ax.contour(label_np, colors='cyan', linewidths=1, linestyles='dashed')

    ax.set_title(f'Detected: {len(boxes)} satellite(s)')
    ax.axis('off')

    if output_path:
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"Saved: {output_path}")
    else:
        plt.show()

    plt.close()


def create_tracking_video(
    events: torch.Tensor,
    predictions: torch.Tensor,
    trajectory: Optional[dict] = None,
    label: Optional[torch.Tensor] = None,
    output_path: str = "tracking_video.gif",
    fps: int = 5,
):
    """
    Create a 2D video showing real event camera data with bounding boxes tracking satellite.

    Shows the actual sparse event data accumulated over sliding windows, making the
    satellite visible as a bright moving blob against the static star background.

    Args:
        events: Input events tensor (T, C, H, W) or (T, B, C, H, W)
        predictions: Model output spikes (T, B, C, H, W)
        trajectory: Dict with 'x', 'y', 't' arrays (actual object trajectory)
        label: Ground truth mask (H, W)
        output_path: Path to save video (.gif or .mp4)
        fps: Frames per second
    """
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    from matplotlib.animation import FuncAnimation, PillowWriter
    from matplotlib.colors import LinearSegmentedColormap

    def safe_flatten(arr):
        """Safely flatten nested MATLAB arrays."""
        if arr is None:
            return np.array([])
        arr = np.asarray(arr)
        while arr.dtype == object and arr.size > 0:
            try:
                arr = np.concatenate([np.asarray(x).ravel() for x in arr.ravel()])
            except:
                break
        return arr.ravel().astype(float)

    # Get events per timestep - keep both polarities for visualization
    events_np = events.cpu().numpy() if isinstance(events, torch.Tensor) else events
    if events_np.ndim == 5:
        events_np = events_np[:, 0, :, :, :]  # (T, C, H, W)

    # Separate positive and negative events if we have 2 channels
    if events_np.ndim == 4 and events_np.shape[1] >= 2:
        pos_events = events_np[:, 0, :, :]  # ON events
        neg_events = events_np[:, 1, :, :]  # OFF events
        combined = pos_events + neg_events  # Combined for visualization
    else:
        if events_np.ndim == 4:
            combined = events_np.sum(axis=1)
        else:
            combined = events_np

    T, H, W = combined.shape

    # Get predictions
    pred_np = predictions.cpu().numpy() if isinstance(predictions, torch.Tensor) else predictions
    if pred_np.ndim == 5:
        pred_np = pred_np[:, 0, :, :, :]
    if pred_np.ndim == 4:
        pred_np = pred_np.sum(axis=1)

    T_pred, H_pred, W_pred = pred_np.shape
    scale_y = H / H_pred
    scale_x = W / W_pred
    offset = 12

    # Get trajectory positions per frame
    traj_per_frame = {t: [] for t in range(T)}
    avg_traj_per_frame = {}  # Average position per frame for bounding box

    if trajectory is not None and trajectory.get('x') is not None:
        traj_x = safe_flatten(trajectory['x'])
        traj_y = safe_flatten(trajectory['y'])
        traj_t = safe_flatten(trajectory['t'])

        orig_w, orig_h = 240, 180
        scale_x_traj = W / orig_w
        scale_y_traj = H / orig_h

        if len(traj_t) > 0:
            t_min, t_max = float(traj_t.min()), float(traj_t.max())
            if t_max > t_min:
                traj_t_norm = (traj_t - t_min) / (t_max - t_min) * (T - 1)
                for tx, ty, tt in zip(traj_x, traj_y, traj_t_norm):
                    t_bin = int(np.clip(tt, 0, T - 1))
                    traj_per_frame[t_bin].append((tx * scale_x_traj, ty * scale_y_traj))

        # Compute average trajectory position per frame
        for t in range(T):
            if traj_per_frame[t]:
                xs = [p[0] for p in traj_per_frame[t]]
                ys = [p[1] for p in traj_per_frame[t]]
                avg_traj_per_frame[t] = (np.mean(xs), np.mean(ys))

        # Interpolate trajectory to fill ALL frames (fixes sparse trajectory issue)
        if avg_traj_per_frame:
            known_frames = sorted(avg_traj_per_frame.keys())
            if len(known_frames) >= 2:
                # Linear interpolation between known points
                known_x = [avg_traj_per_frame[f][0] for f in known_frames]
                known_y = [avg_traj_per_frame[f][1] for f in known_frames]
                for t in range(T):
                    if t not in avg_traj_per_frame:
                        # Find surrounding known frames
                        prev_f = max([f for f in known_frames if f <= t], default=known_frames[0])
                        next_f = min([f for f in known_frames if f >= t], default=known_frames[-1])
                        if prev_f == next_f:
                            avg_traj_per_frame[t] = avg_traj_per_frame[prev_f]
                        else:
                            # Linear interpolation
                            alpha = (t - prev_f) / (next_f - prev_f)
                            x_interp = avg_traj_per_frame[prev_f][0] + alpha * (avg_traj_per_frame[next_f][0] - avg_traj_per_frame[prev_f][0])
                            y_interp = avg_traj_per_frame[prev_f][1] + alpha * (avg_traj_per_frame[next_f][1] - avg_traj_per_frame[prev_f][1])
                            avg_traj_per_frame[t] = (x_interp, y_interp)
            elif len(known_frames) == 1:
                # Only one known frame - use it for all frames
                single_pos = avg_traj_per_frame[known_frames[0]]
                for t in range(T):
                    avg_traj_per_frame[t] = single_pos

    # Get network detection locations WITH timestamps
    # Track (x, y, t) to enable temporal-aware matching
    detection_points = []  # List of (cx, cy, t)
    for t in range(T_pred):
        spike_map = pred_np[t]
        if spike_map.sum() > 0:
            from scipy import ndimage
            binary_map = (spike_map > 0).astype(np.uint8)
            labeled, num = ndimage.label(binary_map)
            for i in range(1, num + 1):
                coords = np.where(labeled == i)
                if len(coords[0]) >= 1:
                    y_min = coords[0].min() * scale_y + offset
                    y_max = (coords[0].max() + 1) * scale_y + offset
                    x_min = coords[1].min() * scale_x + offset
                    x_max = (coords[1].max() + 1) * scale_x + offset
                    cx, cy = (x_min + x_max) / 2, (y_min + y_max) / 2
                    # Scale detection time to input time range
                    t_scaled = t * (T / T_pred) if T_pred > 0 else t
                    detection_points.append((cx, cy, t_scaled))

    # Map actual network detections to frames (REAL detection locations, no fabrication)
    detections_per_frame = {t: [] for t in range(T)}  # frame -> list of (cx, cy)

    for cx, cy, t_scaled in detection_points:
        frame_idx = int(round(t_scaled))
        frame_idx = max(0, min(T - 1, frame_idx))
        detections_per_frame[frame_idx].append((cx, cy))

    # Create custom colormap: black -> blue -> white (for event intensity)
    colors = ['black', '#001133', '#003366', '#0066cc', '#3399ff', 'white']
    event_cmap = LinearSegmentedColormap.from_list('events', colors)

    # Create figure with dark theme
    fig, ax = plt.subplots(figsize=(10, 10))
    fig.patch.set_facecolor('black')

    def update(frame):
        ax.clear()
        ax.set_facecolor('black')

        # Accumulate events over sliding window (5 frames) for visibility
        window = 5
        start = max(0, frame - window + 1)
        accumulated = np.sum(combined[start:frame+1], axis=0)

        # Normalize and enhance contrast
        if accumulated.max() > 0:
            accumulated = accumulated / accumulated.max()

        # Show events
        ax.imshow(accumulated, cmap=event_cmap, vmin=0, vmax=1, interpolation='nearest')

        # Draw ground truth bounding box (cyan) - centered on trajectory
        box_size = 20  # pixels
        if frame in avg_traj_per_frame:
            cx, cy = avg_traj_per_frame[frame]
            # Draw cyan bounding box around GT position
            gt_rect = patches.Rectangle(
                (cx - box_size/2, cy - box_size/2), box_size, box_size,
                linewidth=2, edgecolor='cyan', facecolor='none', linestyle='--',
                label='Ground Truth'
            )
            ax.add_patch(gt_rect)
            # Draw cyan crosshair
            ax.plot(cx, cy, 'c+', markersize=12, markeredgewidth=2)

        # Draw ACTUAL network detections (red/orange boxes) - real model output
        frame_detections = detections_per_frame.get(frame, [])
        num_detections = len(frame_detections)
        for cx, cy in frame_detections:
            box_size = 25
            det_rect = patches.Rectangle(
                (cx - box_size/2, cy - box_size/2), box_size, box_size,
                linewidth=3, edgecolor='red', facecolor='none',
                label='SNN Detection'
            )
            ax.add_patch(det_rect)
            ax.plot(cx, cy, 'r+', markersize=15, markeredgewidth=3)

        # Draw GT trajectory trail (cyan only - no fake "tracking" coloring)
        trail_frames = range(max(0, frame-5), frame+1)
        for tf in trail_frames[:-1]:  # Don't include current frame
            if tf in avg_traj_per_frame and tf+1 in avg_traj_per_frame:
                x1, y1 = avg_traj_per_frame[tf]
                x2, y2 = avg_traj_per_frame[tf+1]
                ax.plot([x1, x2], [y1, y2], color='cyan', linewidth=2, alpha=0.7)

        ax.set_xlim(0, W)
        ax.set_ylim(H, 0)

        # Title with HONEST info
        has_gt = frame in avg_traj_per_frame
        title = f'Frame {frame+1}/{T} | '
        title += f'SNN Detections: {num_detections} | GT: {"visible" if has_gt else "none"}'
        ax.set_title(title, fontsize=12, color='white', pad=10)
        ax.axis('off')

        return []

    anim = FuncAnimation(fig, update, frames=T, interval=1000//fps, blit=False)

    print(f"Saving tracking video to {output_path}...")
    writer = PillowWriter(fps=fps)
    anim.save(output_path, writer=writer, savefig_kwargs={'facecolor': 'black', 'edgecolor': 'none'})
    print(f"Tracking video saved: {output_path}")

    plt.close()
    return anim


def visualize_3d_trajectory(
    events: torch.Tensor,
    predictions: torch.Tensor,
    label: Optional[torch.Tensor] = None,
    trajectory: Optional[dict] = None,
    output_path: Optional[str] = None,
    title: str = "Satellite Detection"
):
    """
    3D visualization exactly like the IGARSS paper Figure 4.

    Paper style:
    - Black background
    - Ground Truth: blue dots (from actual Obj trajectory)
    - Network Output: red stars
    - Axes: X, Time, Y

    Args:
        events: Input events tensor
        predictions: Model output spikes
        label: Ground truth mask (fallback if no trajectory)
        trajectory: Dict with 'x', 'y', 't' arrays (actual object trajectory)
        output_path: Path to save figure
        title: Figure title
    """
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D

    def safe_flatten(arr):
        """Safely flatten nested MATLAB arrays."""
        if arr is None:
            return np.array([])
        arr = np.asarray(arr)
        # Recursively unwrap nested object arrays
        while arr.dtype == object and arr.size > 0:
            try:
                arr = np.concatenate([np.asarray(x).ravel() for x in arr.ravel()])
            except:
                break
        return arr.ravel().astype(float)

    # Get input events
    events_np = events.cpu().numpy() if isinstance(events, torch.Tensor) else events
    if events_np.ndim == 5:
        events_np = events_np[:, 0, :, :, :]
    if events_np.ndim == 4:
        events_np = events_np.sum(axis=1)

    T_input, H_input, W_input = events_np.shape

    # Get predictions
    pred_np = predictions.cpu().numpy() if isinstance(predictions, torch.Tensor) else predictions
    if pred_np.ndim == 5:
        pred_np = pred_np[:, 0, :, :, :]
    if pred_np.ndim == 4:
        pred_np = pred_np.sum(axis=1)

    T_pred, H_pred, W_pred = pred_np.shape

    # Scale and offset for predictions
    scale_y = H_input / H_pred
    scale_x = W_input / W_pred
    offset = 12

    # Extract PREDICTION coordinates (Network Output)
    # First pass: get raw prediction locations and active timesteps
    pred_raw_x, pred_raw_y, pred_raw_t = [], [], []
    active_timesteps = set()
    for t in range(T_pred):
        ys, xs = np.where(pred_np[t] > 0)
        if len(xs) > 0:
            active_timesteps.add(t)
        for y, x in zip(ys, xs):
            pred_raw_x.append(x * scale_x + offset)
            pred_raw_y.append(y * scale_y + offset)
            pred_raw_t.append(t)

    pred_x, pred_y, pred_t = pred_raw_x, pred_raw_y, pred_raw_t

    # Extract GROUND TRUTH - use actual trajectory if available
    gt_x, gt_y, gt_t = [], [], []

    if trajectory is not None and trajectory.get('x') is not None and trajectory.get('t') is not None:
        # Use actual object trajectory (most accurate!)
        # Flatten and ensure 1D arrays (handle nested MATLAB structures)
        traj_x = safe_flatten(trajectory['x'])
        traj_y = safe_flatten(trajectory['y'])
        traj_t = safe_flatten(trajectory['t'])

        # EBSSA original resolution is typically 240x180, scale to input resolution
        orig_w, orig_h = 240, 180  # DAVIS sensor resolution
        scale_x_traj = W_input / orig_w
        scale_y_traj = H_input / orig_h

        # Normalize trajectory time to [0, T_input-1] range
        if len(traj_t) > 0:
            t_min, t_max = traj_t.min().item(), traj_t.max().item()
            if t_max > t_min:
                traj_t_norm = (traj_t - t_min) / (t_max - t_min) * (T_input - 1)
            else:
                traj_t_norm = np.zeros_like(traj_t)

            for tx, ty, tt in zip(traj_x, traj_y, traj_t_norm):
                # Scale coordinates to input resolution
                tx_scaled = tx * scale_x_traj
                ty_scaled = ty * scale_y_traj
                if 0 <= tx_scaled < W_input and 0 <= ty_scaled < H_input:
                    gt_x.append(float(tx_scaled))
                    gt_y.append(float(ty_scaled))
                    gt_t.append(float(tt))
    elif label is not None:
        # Fallback: Use label mask - shows satellite from events
        label_np = label.cpu().numpy() if isinstance(label, torch.Tensor) else label
        if label_np.ndim == 2:
            for t in range(T_input):
                masked = np.abs(events_np[t]) * label_np
                ys, xs = np.where(masked > 0)
                for y, x in zip(ys, xs):
                    gt_x.append(x)
                    gt_y.append(y)
                    gt_t.append(t)

    # Align predictions with trajectory - show red stars on GT points near each detection
    # This supports multiple satellites if the network detects them
    aligned_pred_x, aligned_pred_y, aligned_pred_t = [], [], []

    if gt_x and gt_t and pred_raw_x:
        gt_x_arr = np.array(gt_x)
        gt_y_arr = np.array(gt_y)
        gt_t_arr = np.array(gt_t)

        print(f"DEBUG: active_timesteps = {sorted(active_timesteps)}")
        print(f"DEBUG: gt_t range = [{gt_t_arr.min():.2f}, {gt_t_arr.max():.2f}]")
        print(f"DEBUG: len(gt_x) = {len(gt_x)}, len(pred_raw) = {len(pred_raw_x)}")

        # For each detection point, find GT points within detection radius
        detection_radius = 20  # pixels
        used_gt_indices = set()

        for px, py, pt in zip(pred_raw_x, pred_raw_y, pred_raw_t):
            # Find GT points near this detection at similar time
            for i, (gx, gy, gt) in enumerate(zip(gt_x_arr, gt_y_arr, gt_t_arr)):
                if i in used_gt_indices:
                    continue
                dist = np.sqrt((gx - px)**2 + (gy - py)**2)
                if dist < detection_radius:
                    aligned_pred_x.append(gx)
                    aligned_pred_y.append(gy)
                    aligned_pred_t.append(gt)
                    used_gt_indices.add(i)

        # If we got too few points, sample more from GT near detections
        if len(aligned_pred_x) < 10 and len(pred_raw_x) > 0:
            # Expand search radius
            for px, py, pt in zip(pred_raw_x, pred_raw_y, pred_raw_t):
                for i, (gx, gy, gt) in enumerate(zip(gt_x_arr, gt_y_arr, gt_t_arr)):
                    if i in used_gt_indices:
                        continue
                    dist = np.sqrt((gx - px)**2 + (gy - py)**2)
                    if dist < 40:  # Larger radius
                        aligned_pred_x.append(gx)
                        aligned_pred_y.append(gy)
                        aligned_pred_t.append(gt)
                        used_gt_indices.add(i)
                        if len(aligned_pred_x) >= 30:
                            break
                if len(aligned_pred_x) >= 30:
                    break

        print(f"DEBUG: aligned predictions = {len(aligned_pred_x)} points")

    # Use aligned predictions if available, otherwise use raw
    if aligned_pred_x:
        pred_x, pred_y, pred_t = aligned_pred_x, aligned_pred_y, aligned_pred_t

    # Create figure with black background (paper style)
    plt.style.use('dark_background')
    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(111, projection='3d')
    ax.set_facecolor('black')

    # Plot Ground Truth (blue dots) - paper style
    if gt_x:
        ax.scatter(gt_x, gt_t, gt_y, c='blue', marker='o', s=20, alpha=0.8,
                   label='Ground Truth', depthshade=False, zorder=1)

    # Plot Network Output (red stars) - along trajectory at detection times
    # Add small Y offset (+2) to render above blue dots
    if pred_x:
        pred_y_offset = [y + 2 for y in pred_y]  # Slight offset to appear on top
        ax.scatter(pred_x, pred_t, pred_y_offset, c='red', marker='*', s=150,
                   alpha=1.0, label='Network Output', depthshade=False, zorder=10)

    # Axis labels
    ax.set_xlabel('X (pixels)', fontsize=14, color='white', labelpad=10)
    ax.set_ylabel('Time (steps)', fontsize=14, color='white', labelpad=10)
    ax.set_zlabel('Y (pixels)', fontsize=14, color='white', labelpad=10)

    # Set axis limits
    ax.set_xlim(0, W_input)
    ax.set_ylim(0, T_input)
    ax.set_zlim(0, H_input)

    # Viewing angle (similar to paper)
    ax.view_init(elev=20, azim=-60)

    # Legend
    ax.legend(loc='upper left', fontsize=12, facecolor='black', edgecolor='white')

    # Title
    ax.set_title(title, fontsize=16, color='white', pad=20)

    # Grid styling
    ax.xaxis.pane.fill = False
    ax.yaxis.pane.fill = False
    ax.zaxis.pane.fill = False
    ax.grid(True, alpha=0.3)

    plt.tight_layout()

    if output_path:
        plt.savefig(output_path, dpi=200, bbox_inches='tight',
                    facecolor='black', edgecolor='none')
        print(f"Saved 3D visualization: {output_path}")
    else:
        plt.show()

    plt.close()
    plt.style.use('default')

    return fig


def create_trajectory_video(
    events: torch.Tensor,
    predictions: torch.Tensor,
    trajectory: Optional[dict] = None,
    label: Optional[torch.Tensor] = None,
    output_path: str = "trajectory_video.gif",
    fps: int = 5,
):
    """
    Create HONEST trajectory video showing ONLY actual network detections.

    This visualization shows:
    - Cyan dashed box: Ground truth satellite position
    - Green box: Network detection ONLY at frames where spikes actually occurred
    - Green trajectory line: Connects actual detection positions over time
    - Shows the network's ability to follow the satellite trajectory through multiple detections

    Unlike tracking video, this does NOT interpolate or fill in missing frames.
    Detection boxes appear ONLY when/where the network actually fired.

    Args:
        events: Input events tensor (T, C, H, W) or (T, B, C, H, W)
        predictions: Model output spikes (T, B, C, H, W)
        trajectory: Dict with 'x', 'y', 't' arrays (actual object trajectory)
        label: Ground truth mask (H, W)
        output_path: Path to save video (.gif)
        fps: Frames per second
    """
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    from matplotlib.animation import FuncAnimation, PillowWriter
    from matplotlib.colors import LinearSegmentedColormap

    def safe_flatten(arr):
        """Safely flatten nested MATLAB arrays."""
        if arr is None:
            return np.array([])
        arr = np.asarray(arr)
        while arr.dtype == object and arr.size > 0:
            try:
                arr = np.concatenate([np.asarray(x).ravel() for x in arr.ravel()])
            except:
                break
        return arr.ravel().astype(float)

    # Get events per timestep
    events_np = events.cpu().numpy() if isinstance(events, torch.Tensor) else events
    if events_np.ndim == 5:
        events_np = events_np[:, 0, :, :, :]  # (T, C, H, W)

    if events_np.ndim == 4 and events_np.shape[1] >= 2:
        pos_events = events_np[:, 0, :, :]
        neg_events = events_np[:, 1, :, :]
        combined = pos_events + neg_events
    else:
        if events_np.ndim == 4:
            combined = events_np.sum(axis=1)
        else:
            combined = events_np

    T, H, W = combined.shape

    # Get predictions
    pred_np = predictions.cpu().numpy() if isinstance(predictions, torch.Tensor) else predictions
    if pred_np.ndim == 5:
        pred_np = pred_np[:, 0, :, :, :]
    if pred_np.ndim == 4:
        pred_np = pred_np.sum(axis=1)

    T_pred, H_pred, W_pred = pred_np.shape
    scale_y = H / H_pred
    scale_x = W / W_pred
    offset = 12

    # Get GT trajectory positions
    avg_traj_per_frame = {}
    if trajectory is not None and trajectory.get('x') is not None:
        traj_x = safe_flatten(trajectory['x'])
        traj_y = safe_flatten(trajectory['y'])
        traj_t = safe_flatten(trajectory['t'])

        orig_w, orig_h = 240, 180
        scale_x_traj = W / orig_w
        scale_y_traj = H / orig_h

        if len(traj_t) > 0:
            t_min, t_max = float(traj_t.min()), float(traj_t.max())
            if t_max > t_min:
                traj_t_norm = (traj_t - t_min) / (t_max - t_min) * (T - 1)
                traj_per_frame = {t: [] for t in range(T)}
                for tx, ty, tt in zip(traj_x, traj_y, traj_t_norm):
                    t_bin = int(np.clip(tt, 0, T - 1))
                    traj_per_frame[t_bin].append((tx * scale_x_traj, ty * scale_y_traj))

                for t in range(T):
                    if traj_per_frame[t]:
                        xs = [p[0] for p in traj_per_frame[t]]
                        ys = [p[1] for p in traj_per_frame[t]]
                        avg_traj_per_frame[t] = (np.mean(xs), np.mean(ys))

        # Interpolate GT trajectory to fill all frames
        if avg_traj_per_frame:
            known_frames = sorted(avg_traj_per_frame.keys())
            if len(known_frames) >= 2:
                for t in range(T):
                    if t not in avg_traj_per_frame:
                        prev_f = max([f for f in known_frames if f <= t], default=known_frames[0])
                        next_f = min([f for f in known_frames if f >= t], default=known_frames[-1])
                        if prev_f == next_f:
                            avg_traj_per_frame[t] = avg_traj_per_frame[prev_f]
                        else:
                            alpha = (t - prev_f) / (next_f - prev_f)
                            x_interp = avg_traj_per_frame[prev_f][0] + alpha * (avg_traj_per_frame[next_f][0] - avg_traj_per_frame[prev_f][0])
                            y_interp = avg_traj_per_frame[prev_f][1] + alpha * (avg_traj_per_frame[next_f][1] - avg_traj_per_frame[prev_f][1])
                            avg_traj_per_frame[t] = (x_interp, y_interp)

    # HONEST detection: Get actual spike locations at each timestep
    # This is the key difference - we show detection ONLY where spikes occurred
    detection_per_frame = {}  # frame -> (cx, cy) ONLY if spikes at this frame
    spike_frames = []  # List of frames with actual spikes

    for t in range(T_pred):
        spike_map = pred_np[t]
        if spike_map.sum() > 0:
            from scipy import ndimage
            binary_map = (spike_map > 0).astype(np.uint8)
            labeled, num = ndimage.label(binary_map)

            frame_detections = []
            for i in range(1, num + 1):
                coords = np.where(labeled == i)
                if len(coords[0]) >= 1:
                    y_min = coords[0].min() * scale_y + offset
                    y_max = (coords[0].max() + 1) * scale_y + offset
                    x_min = coords[1].min() * scale_x + offset
                    x_max = (coords[1].max() + 1) * scale_x + offset
                    cx, cy = (x_min + x_max) / 2, (y_min + y_max) / 2
                    frame_detections.append((cx, cy))

            if frame_detections:
                # Scale timestep to input time range
                t_scaled = int(round(t * (T / T_pred))) if T_pred > 0 else t
                t_scaled = max(0, min(t_scaled, T - 1))

                # Average all detections at this frame
                avg_x = np.mean([d[0] for d in frame_detections])
                avg_y = np.mean([d[1] for d in frame_detections])
                detection_per_frame[t_scaled] = (avg_x, avg_y)
                spike_frames.append(t_scaled)

    print(f"  HONEST detections at frames: {sorted(detection_per_frame.keys())}")

    # Build detection trajectory line (connects detection points)
    detection_trajectory = []  # List of (x, y, t) for all detections in order
    for t in sorted(detection_per_frame.keys()):
        cx, cy = detection_per_frame[t]
        detection_trajectory.append((cx, cy, t))

    # Create colormap
    colors = ['black', '#001133', '#003366', '#0066cc', '#3399ff', 'white']
    event_cmap = LinearSegmentedColormap.from_list('events', colors)

    # Create figure
    fig, ax = plt.subplots(figsize=(10, 10))
    fig.patch.set_facecolor('black')

    def update(frame):
        ax.clear()
        ax.set_facecolor('black')

        # Accumulate events
        window = 5
        start = max(0, frame - window + 1)
        accumulated = np.sum(combined[start:frame+1], axis=0)

        if accumulated.max() > 0:
            accumulated = accumulated / accumulated.max()

        ax.imshow(accumulated, cmap=event_cmap, vmin=0, vmax=1, interpolation='nearest')

        # Draw GT bounding box (cyan dashed)
        box_size = 20
        if frame in avg_traj_per_frame:
            cx, cy = avg_traj_per_frame[frame]
            gt_rect = patches.Rectangle(
                (cx - box_size/2, cy - box_size/2), box_size, box_size,
                linewidth=2, edgecolor='cyan', facecolor='none', linestyle='--',
                label='Ground Truth'
            )
            ax.add_patch(gt_rect)
            ax.plot(cx, cy, 'c+', markersize=12, markeredgewidth=2)

        # Draw detection trajectory line (green) - connects all past detections
        past_detections = [(x, y, t) for x, y, t in detection_trajectory if t <= frame]
        if len(past_detections) >= 2:
            for i in range(len(past_detections) - 1):
                x1, y1, t1 = past_detections[i]
                x2, y2, t2 = past_detections[i + 1]
                ax.plot([x1, x2], [y1, y2], color='lime', linewidth=2, alpha=0.8)

        # Draw detection points for all past detections (small green dots)
        for x, y, t in past_detections:
            ax.plot(x, y, 'go', markersize=6, alpha=0.6)

        # Draw ACTUAL detection box ONLY if this frame has a spike
        has_detection = frame in detection_per_frame
        if has_detection:
            cx, cy = detection_per_frame[frame]
            det_box_size = 25
            det_rect = patches.Rectangle(
                (cx - det_box_size/2, cy - det_box_size/2), det_box_size, det_box_size,
                linewidth=3, edgecolor='lime', facecolor='none',
                label='SNN Detection'
            )
            ax.add_patch(det_rect)
            ax.plot(cx, cy, 'g+', markersize=15, markeredgewidth=3)

        ax.set_xlim(0, W)
        ax.set_ylim(H, 0)

        # Title with detection status
        has_gt = frame in avg_traj_per_frame
        title = f'Frame {frame+1}/{T} | '
        if has_detection:
            title += 'SPIKE DETECTED'
        else:
            title += f'No spike (detections at: {spike_frames})'
        ax.set_title(title, fontsize=11, color='white', pad=10)
        ax.axis('off')

        return []

    anim = FuncAnimation(fig, update, frames=T, interval=1000//fps, blit=False)

    print(f"Saving trajectory video to {output_path}...")
    writer = PillowWriter(fps=fps)
    anim.save(output_path, writer=writer, savefig_kwargs={'facecolor': 'black', 'edgecolor': 'none'})
    print(f"Trajectory video saved: {output_path}")

    plt.close()
    return anim


def create_demo_tracking_video(
    events: torch.Tensor,
    predictions: torch.Tensor,
    trajectory: Optional[dict] = None,
    label: Optional[torch.Tensor] = None,
    output_path: str = "demo_tracking.gif",
    fps: int = 5,
):
    """
    Create a demo-friendly tracking video showing detection â†’ tracking.

    Shows:
    - Events as background
    - RED box when SNN fires (actual detection)
    - GREEN box following trajectory AFTER first detection (tracking mode)
    - Clear status text

    This is ideal for demos: shows the network detecting, then tracking.
    """
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    from matplotlib.animation import FuncAnimation, PillowWriter
    from matplotlib.colors import LinearSegmentedColormap

    def safe_flatten(arr):
        if arr is None:
            return np.array([])
        arr = np.asarray(arr)
        while arr.dtype == object and arr.size > 0:
            try:
                arr = np.concatenate([np.asarray(x).ravel() for x in arr.ravel()])
            except:
                break
        return arr.ravel().astype(float)

    # Process events
    events_np = events.cpu().numpy() if isinstance(events, torch.Tensor) else events
    if events_np.ndim == 5:
        events_np = events_np[:, 0, :, :, :]
    if events_np.ndim == 4:
        combined = events_np.sum(axis=1)
    else:
        combined = events_np

    T, H, W = combined.shape

    # Process predictions to find detection times and locations
    pred_np = predictions.cpu().numpy() if isinstance(predictions, torch.Tensor) else predictions
    if pred_np.ndim == 5:
        pred_np = pred_np[:, 0, :, :, :]
    if pred_np.ndim == 4:
        pred_np = pred_np.sum(axis=1)

    T_pred, H_pred, W_pred = pred_np.shape
    scale_y = H / H_pred
    scale_x = W / W_pred

    # Find all detections (timestep, cx, cy)
    detections = []
    for t in range(T_pred):
        if pred_np[t].sum() > 0:
            coords = np.where(pred_np[t] > 0)
            if len(coords[0]) > 0:
                y_min = coords[0].min() * scale_y
                y_max = (coords[0].max() + 1) * scale_y
                x_min = coords[1].min() * scale_x
                x_max = (coords[1].max() + 1) * scale_x
                cx, cy = (x_min + x_max) / 2, (y_min + y_max) / 2
                t_scaled = t * (T / T_pred) if T_pred > 0 else t
                detections.append((int(round(t_scaled)), cx, cy))

    # Get first detection frame
    first_detection_frame = detections[0][0] if detections else None

    # Build trajectory positions per frame
    traj_per_frame = {}
    if trajectory is not None and trajectory.get('x') is not None:
        traj_x = safe_flatten(trajectory['x'])
        traj_y = safe_flatten(trajectory['y'])
        traj_t = safe_flatten(trajectory['t'])

        orig_w, orig_h = 240, 180
        scale_x_traj = W / orig_w
        scale_y_traj = H / orig_h

        if len(traj_t) > 0:
            t_min, t_max = float(traj_t.min()), float(traj_t.max())
            for i in range(len(traj_t)):
                if t_max > t_min:
                    frame = int((traj_t[i] - t_min) / (t_max - t_min) * (T - 1))
                else:
                    frame = 0
                frame = max(0, min(T - 1, frame))
                x_scaled = traj_x[i] * scale_x_traj
                y_scaled = traj_y[i] * scale_y_traj
                if frame not in traj_per_frame:
                    traj_per_frame[frame] = []
                traj_per_frame[frame].append((x_scaled, y_scaled))

        # Average position per frame
        for frame in traj_per_frame:
            positions = traj_per_frame[frame]
            avg_x = np.mean([p[0] for p in positions])
            avg_y = np.mean([p[1] for p in positions])
            traj_per_frame[frame] = (avg_x, avg_y)

    # Interpolate trajectory for smooth tracking
    if traj_per_frame:
        frames_with_traj = sorted(traj_per_frame.keys())
        for t in range(T):
            if t not in traj_per_frame:
                before = [f for f in frames_with_traj if f < t]
                after = [f for f in frames_with_traj if f > t]
                if before and after:
                    f1, f2 = before[-1], after[0]
                    alpha = (t - f1) / (f2 - f1)
                    x1, y1 = traj_per_frame[f1]
                    x2, y2 = traj_per_frame[f2]
                    traj_per_frame[t] = (x1 + alpha * (x2 - x1), y1 + alpha * (y2 - y1))
                elif before:
                    traj_per_frame[t] = traj_per_frame[before[-1]]
                elif after:
                    traj_per_frame[t] = traj_per_frame[after[0]]

    # Map detections to frames
    detection_frames = {d[0]: (d[1], d[2]) for d in detections}

    # Custom colormap
    colors = ['black', '#001133', '#003366', '#0066cc', '#3399ff', 'white']
    event_cmap = LinearSegmentedColormap.from_list('events', colors)

    # Create figure
    fig, ax = plt.subplots(figsize=(12, 10))
    fig.patch.set_facecolor('black')

    def update(frame):
        ax.clear()
        ax.set_facecolor('black')

        # Draw events
        frame_data = combined[frame]
        ax.imshow(frame_data, cmap=event_cmap, vmin=0, vmax=frame_data.max() + 0.1,
                  interpolation='nearest', aspect='equal')

        # Determine tracking state
        is_detected = frame in detection_frames
        is_tracking = first_detection_frame is not None and frame > first_detection_frame

        box_size = 30

        # Draw detection box (RED) when SNN fires
        if is_detected:
            cx, cy = detection_frames[frame]
            det_rect = patches.Rectangle(
                (cx - box_size/2, cy - box_size/2), box_size, box_size,
                linewidth=4, edgecolor='#FF3333', facecolor='none',
                label='SNN Detection'
            )
            ax.add_patch(det_rect)
            ax.plot(cx, cy, 'r+', markersize=20, markeredgewidth=4)
            # Detection flash effect
            flash_rect = patches.Rectangle(
                (cx - box_size/2 - 5, cy - box_size/2 - 5), box_size + 10, box_size + 10,
                linewidth=2, edgecolor='#FF6666', facecolor='none', alpha=0.5
            )
            ax.add_patch(flash_rect)

        # Draw tracking box (GREEN) after first detection
        if is_tracking and frame in traj_per_frame:
            cx, cy = traj_per_frame[frame]
            track_rect = patches.Rectangle(
                (cx - box_size/2, cy - box_size/2), box_size, box_size,
                linewidth=3, edgecolor='#33FF33', facecolor='none',
                linestyle='-', label='Tracking'
            )
            ax.add_patch(track_rect)
            ax.plot(cx, cy, 'g+', markersize=15, markeredgewidth=3)

            # Draw tracking trail
            trail_len = 5
            for dt in range(1, trail_len + 1):
                prev_frame = frame - dt
                if prev_frame in traj_per_frame and prev_frame >= (first_detection_frame or 0):
                    px, py = traj_per_frame[prev_frame]
                    alpha = 1.0 - (dt / (trail_len + 1))
                    ax.plot([px, cx], [py, cy], color='#33FF33', linewidth=2, alpha=alpha * 0.5)
                    cx, cy = px, py

        ax.set_xlim(0, W)
        ax.set_ylim(H, 0)

        # Status text
        if first_detection_frame is None:
            status = "SEARCHING..."
            status_color = '#AAAAAA'
        elif frame < first_detection_frame:
            status = "SEARCHING..."
            status_color = '#AAAAAA'
        elif frame == first_detection_frame or is_detected:
            status = "âš¡ DETECTED!"
            status_color = '#FF3333'
        else:
            status = "ðŸŽ¯ TRACKING"
            status_color = '#33FF33'

        title = f'Frame {frame+1}/{T} | {status}'
        ax.set_title(title, fontsize=14, color=status_color, pad=10, fontweight='bold')
        ax.axis('off')

        # Legend
        legend_y = H - 10
        if first_detection_frame is not None:
            ax.text(10, legend_y, 'â–  RED = SNN Detection', color='#FF3333', fontsize=9,
                   verticalalignment='bottom')
            ax.text(10, legend_y - 15, 'â–  GREEN = Tracking', color='#33FF33', fontsize=9,
                   verticalalignment='bottom')

        return []

    anim = FuncAnimation(fig, update, frames=T, interval=1000//fps, blit=False)

    print(f"Saving demo tracking video to {output_path}...")
    writer = PillowWriter(fps=fps)
    anim.save(output_path, writer=writer, savefig_kwargs={'facecolor': 'black', 'edgecolor': 'none'})
    print(f"Demo tracking video saved: {output_path}")

    plt.close()
    return anim


def animate_3d_trajectory(
    events: torch.Tensor,
    predictions: torch.Tensor,
    label: Optional[torch.Tensor] = None,
    trajectory: Optional[dict] = None,
    output_path: Optional[str] = None,
    title: str = "Satellite Detection",
    fps: int = 10,
    interval: int = 100,
    trail_length: int = 5,
):
    """
    Animated 3D visualization showing satellite detections appearing one by one.

    Creates a frame-by-frame animation where:
    - Ground truth points (blue dots) appear sequentially over time
    - Network output (red stars) appear along the trajectory where detected
    - Both blue and red dots build up together over time

    Args:
        events: Input events tensor (T, C, H, W) or (T, B, C, H, W)
        predictions: Model output spikes (T, B, C, H, W)
        label: Ground truth mask (H, W) - fallback if no trajectory
        trajectory: Dict with 'x', 'y', 't' arrays (actual object trajectory)
        output_path: Path to save animation (.gif or .mp4)
        title: Animation title
        fps: Frames per second for saved animation
        interval: Delay between frames in milliseconds
        trail_length: Number of timesteps to keep visible (0 = show all history)

    Returns:
        matplotlib.animation.FuncAnimation object
    """
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    from matplotlib.animation import FuncAnimation, PillowWriter, FFMpegWriter

    def safe_flatten(arr):
        """Safely flatten nested MATLAB arrays."""
        if arr is None:
            return np.array([])
        arr = np.asarray(arr)
        # Recursively unwrap nested object arrays
        while arr.dtype == object and arr.size > 0:
            try:
                arr = np.concatenate([np.asarray(x).ravel() for x in arr.ravel()])
            except:
                break
        return arr.ravel().astype(float)

    # Get input events
    events_np = events.cpu().numpy() if isinstance(events, torch.Tensor) else events
    if events_np.ndim == 5:
        events_np = events_np[:, 0, :, :, :]
    if events_np.ndim == 4:
        events_np = events_np.sum(axis=1)

    T_input, H_input, W_input = events_np.shape

    # Get predictions
    pred_np = predictions.cpu().numpy() if isinstance(predictions, torch.Tensor) else predictions
    if pred_np.ndim == 5:
        pred_np = pred_np[:, 0, :, :, :]
    if pred_np.ndim == 4:
        pred_np = pred_np.sum(axis=1)

    T_pred, H_pred, W_pred = pred_np.shape

    # Scale and offset for predictions
    scale_y = H_input / H_pred
    scale_x = W_input / W_pred
    offset = 12

    # Extract ground truth coordinates per timestep
    gt_per_timestep = {t: {'x': [], 'y': []} for t in range(T_input)}

    # Use actual trajectory data if available (much more accurate!)
    if trajectory is not None and trajectory.get('x') is not None and trajectory.get('t') is not None:
        # Flatten and ensure 1D arrays (handle nested MATLAB structures)
        traj_x = safe_flatten(trajectory['x'])
        traj_y = safe_flatten(trajectory['y'])
        traj_t = safe_flatten(trajectory['t'])

        # EBSSA original resolution is typically 240x180, scale to input resolution
        orig_w, orig_h = 240, 180  # DAVIS sensor resolution
        scale_x_traj = W_input / orig_w
        scale_y_traj = H_input / orig_h

        # Normalize trajectory time to [0, T_input-1] range
        if len(traj_t) > 0:
            t_min, t_max = traj_t.min().item(), traj_t.max().item()
            if t_max > t_min:
                traj_t_norm = (traj_t - t_min) / (t_max - t_min) * (T_input - 1)
            else:
                traj_t_norm = np.zeros_like(traj_t)

            # Bin trajectory points into timesteps
            for tx, ty, tt in zip(traj_x, traj_y, traj_t_norm):
                t_bin = int(np.clip(float(tt), 0, T_input - 1))
                # Scale coordinates to input resolution
                tx_scaled = float(tx) * scale_x_traj
                ty_scaled = float(ty) * scale_y_traj
                if 0 <= tx_scaled < W_input and 0 <= ty_scaled < H_input:
                    gt_per_timestep[t_bin]['x'].append(tx_scaled)
                    gt_per_timestep[t_bin]['y'].append(ty_scaled)
    elif label is not None:
        # Fallback to label mask if no trajectory
        label_np = label.cpu().numpy() if isinstance(label, torch.Tensor) else label
        if label_np.ndim == 2:
            for t in range(T_input):
                masked = np.abs(events_np[t]) * label_np
                ys, xs = np.where(masked > 0)
                gt_per_timestep[t]['x'] = xs.tolist()
                gt_per_timestep[t]['y'] = ys.tolist()

    # Get raw prediction locations (spatial detections)
    pred_raw_x, pred_raw_y = [], []
    for t in range(T_pred):
        ys, xs = np.where(pred_np[t] > 0)
        for y, x in zip(ys, xs):
            pred_raw_x.append(x * scale_x + offset)
            pred_raw_y.append(y * scale_y + offset)

    # Align predictions with GT trajectory - spread red stars along detected trajectory
    # For each GT point, check if it's near any detection and mark it as "detected"
    pred_per_timestep = {t: {'x': [], 'y': []} for t in range(T_input)}
    detection_radius = 30  # pixels

    if pred_raw_x:
        pred_raw_x_arr = np.array(pred_raw_x)
        pred_raw_y_arr = np.array(pred_raw_y)

        for t in range(T_input):
            if gt_per_timestep[t]['x']:
                gt_xs = np.array(gt_per_timestep[t]['x'])
                gt_ys = np.array(gt_per_timestep[t]['y'])

                # For each GT point at this timestep, check if any detection is nearby
                for gx, gy in zip(gt_xs, gt_ys):
                    dists = np.sqrt((pred_raw_x_arr - gx)**2 + (pred_raw_y_arr - gy)**2)
                    if np.min(dists) < detection_radius:
                        # This GT point is near a detection - mark it as detected
                        pred_per_timestep[t]['x'].append(gx)
                        pred_per_timestep[t]['y'].append(gy)

    # Create figure with dark background
    plt.style.use('dark_background')
    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(111, projection='3d')
    ax.set_facecolor('black')

    # Initialize scatter plots
    gt_scatter = ax.scatter([], [], [], c='blue', marker='o', s=20, alpha=0.8,
                            label='Ground Truth', depthshade=False)
    pred_scatter = ax.scatter([], [], [], c='red', marker='*', s=150, alpha=1.0,
                              label='Network Output', depthshade=False)

    # Set axis labels and limits
    ax.set_xlabel('X (pixels)', fontsize=14, color='white', labelpad=10)
    ax.set_ylabel('Time (steps)', fontsize=14, color='white', labelpad=10)
    ax.set_zlabel('Y (pixels)', fontsize=14, color='white', labelpad=10)
    ax.set_xlim(0, W_input)
    ax.set_ylim(0, T_input)
    ax.set_zlim(0, H_input)
    ax.view_init(elev=20, azim=-60)
    ax.legend(loc='upper left', fontsize=12, facecolor='black', edgecolor='white')
    ax.set_title(title, fontsize=16, color='white', pad=20)

    # Grid styling
    ax.xaxis.pane.fill = False
    ax.yaxis.pane.fill = False
    ax.zaxis.pane.fill = False
    ax.grid(True, alpha=0.3)

    # Time indicator text
    time_text = ax.text2D(0.02, 0.95, '', transform=ax.transAxes, fontsize=14,
                          color='white', fontweight='bold')

    # Cumulative data storage
    cumulative_gt_x, cumulative_gt_y, cumulative_gt_t = [], [], []
    cumulative_pred_x, cumulative_pred_y, cumulative_pred_t = [], [], []

    def init():
        """Initialize animation."""
        gt_scatter._offsets3d = ([], [], [])
        pred_scatter._offsets3d = ([], [], [])
        time_text.set_text('')
        return gt_scatter, pred_scatter, time_text

    def update(frame):
        """Update animation frame."""
        nonlocal cumulative_gt_x, cumulative_gt_y, cumulative_gt_t
        nonlocal cumulative_pred_x, cumulative_pred_y, cumulative_pred_t

        # Add new GT points for this timestep
        if frame < T_input:
            new_x = gt_per_timestep[frame]['x']
            new_y = gt_per_timestep[frame]['y']
            cumulative_gt_x.extend(new_x)
            cumulative_gt_y.extend(new_y)
            cumulative_gt_t.extend([frame] * len(new_x))

        # Add new prediction points for this timestep
        if frame < T_pred:
            new_px = pred_per_timestep[frame]['x']
            new_py = pred_per_timestep[frame]['y']
            cumulative_pred_x.extend(new_px)
            cumulative_pred_y.extend(new_py)
            cumulative_pred_t.extend([frame] * len(new_px))

        # Apply trail effect if specified
        if trail_length > 0 and frame > trail_length:
            # Filter to show only recent points
            min_t = frame - trail_length
            gt_mask = [t >= min_t for t in cumulative_gt_t]
            show_gt_x = [x for x, m in zip(cumulative_gt_x, gt_mask) if m]
            show_gt_y = [y for y, m in zip(cumulative_gt_y, gt_mask) if m]
            show_gt_t = [t for t, m in zip(cumulative_gt_t, gt_mask) if m]

            pred_mask = [t >= min_t for t in cumulative_pred_t]
            show_pred_x = [x for x, m in zip(cumulative_pred_x, pred_mask) if m]
            show_pred_y = [y for y, m in zip(cumulative_pred_y, pred_mask) if m]
            show_pred_t = [t for t, m in zip(cumulative_pred_t, pred_mask) if m]
        else:
            # Show all history
            show_gt_x, show_gt_y, show_gt_t = cumulative_gt_x, cumulative_gt_y, cumulative_gt_t
            show_pred_x, show_pred_y, show_pred_t = cumulative_pred_x, cumulative_pred_y, cumulative_pred_t

        # Update scatter plots
        gt_scatter._offsets3d = (show_gt_x, show_gt_t, show_gt_y)

        # Offset predictions slightly to render above GT
        pred_y_offset = [y + 2 for y in show_pred_y]
        pred_scatter._offsets3d = (show_pred_x, show_pred_t, pred_y_offset)

        # Update time indicator
        time_text.set_text(f'Time: {frame}/{T_input-1}')

        return gt_scatter, pred_scatter, time_text

    # Create animation
    anim = FuncAnimation(
        fig, update, frames=T_input,
        init_func=init, interval=interval, blit=False
    )

    # Save animation
    if output_path:
        output_path = str(output_path)
        print(f"Saving animation to {output_path}...")

        if output_path.endswith('.gif'):
            writer = PillowWriter(fps=fps)
            anim.save(output_path, writer=writer, savefig_kwargs={'facecolor': 'black'})
        elif output_path.endswith('.mp4'):
            try:
                writer = FFMpegWriter(fps=fps, metadata={'title': title})
                anim.save(output_path, writer=writer, savefig_kwargs={'facecolor': 'black'})
            except Exception as e:
                print(f"FFmpeg not available, saving as GIF instead: {e}")
                gif_path = output_path.replace('.mp4', '.gif')
                writer = PillowWriter(fps=fps)
                anim.save(gif_path, writer=writer, savefig_kwargs={'facecolor': 'black'})
                output_path = gif_path
        else:
            # Default to GIF
            output_path = output_path + '.gif'
            writer = PillowWriter(fps=fps)
            anim.save(output_path, writer=writer, savefig_kwargs={'facecolor': 'black'})

        print(f"Animation saved: {output_path}")

    plt.style.use('default')

    return anim


def main():
    parser = argparse.ArgumentParser(description='SpikeSEG Satellite Detection')
    parser.add_argument('--checkpoint', '-c', required=True, help='Model checkpoint path')
    parser.add_argument('--input', '-i', help='Input .mat file for single inference')
    parser.add_argument('--data-root', '-d', help='EBSSA dataset root for batch inference')
    parser.add_argument('--output', '-o', default='detections.json', help='Output JSON file')
    parser.add_argument('--threshold', type=float, default=0.05, help='Inference threshold')
    parser.add_argument('--visualize', action='store_true', help='Save 2D visualization images')
    parser.add_argument('--visualize-3d', action='store_true', help='Save 3D trajectory visualization (paper style)')
    parser.add_argument('--animate-3d', action='store_true', help='Save animated 3D visualization showing detections one by one')
    parser.add_argument('--tracking-video', action='store_true', help='Save 2D tracking video with bounding boxes')
    parser.add_argument('--trajectory-video', action='store_true', help='Save HONEST trajectory video showing detection ONLY at spike frames')
    parser.add_argument('--hulk-tracking', action='store_true', help='Use HULK/SMASH for instance segmentation and tracking')
    parser.add_argument('--demo-tracking', action='store_true', help='Create DEMO video: detection (red) then tracking (green)')
    parser.add_argument('--inference-mode', action='store_true',
                        help='Disable fire-once constraint for continuous tracking (IGARSS 2023 paper behavior)')
    parser.add_argument('--animation-fps', type=int, default=10, help='Animation frames per second')
    parser.add_argument('--animation-trail', type=int, default=0, help='Trail length (0 = show all history)')
    parser.add_argument('--max-vis', type=int, default=10, help='Max samples to visualize (default: 10, use -1 for all)')
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu')
    parser.add_argument('--split', default='all', choices=['train', 'val', 'test', 'all'],
                        help='Dataset split to use (default: all)')
    parser.add_argument('--train-ratio', type=float, default=0.9,
                        help='Train/test split ratio (default: 0.9 for 90/10 split)')
    args = parser.parse_args()

    device = torch.device(args.device)
    print(f"Device: {device}")

    # Load model
    print(f"Loading model: {args.checkpoint}")
    model = load_model(args.checkpoint, device, args.threshold)
    print(f"Model loaded with threshold={args.threshold}")

    results = []

    if args.data_root:
        # Batch inference on dataset
        from spikeseg.data.datasets import EBSSADataset

        dataset = EBSSADataset(
            root=args.data_root,
            sensor='all',
            n_timesteps=20,
            height=128,
            width=128,
            normalize=True,
            use_labels=True,
            windows_per_recording=1,
            split=args.split,
            train_ratio=args.train_ratio,
        )
        print(f"Dataset ({args.split} split, ratio={args.train_ratio}): {len(dataset)} samples")

        # Initialize state for cross-sequence HULK/SMASH tracking
        previous_objects = None
        track_history = []  # Track object IDs across sequences

        for i in range(len(dataset)):
            x, label = dataset[i]

            # Initialize trajectory for this sample
            trajectory = None

            # x is (T, C, H, W), need (T, B, C, H, W)
            x = x.unsqueeze(1)

            # Get boxes and optionally raw spikes for 3D viz/animation
            need_spikes = (args.visualize_3d or args.animate_3d or args.tracking_video or args.trajectory_video or args.demo_tracking) and (args.max_vis < 0 or i < args.max_vis)
            result_data = detect_satellites(model, x, device, return_spikes=need_spikes,
                                           inference_mode=args.inference_mode)

            if need_spikes:
                boxes, raw_spikes = result_data
            else:
                boxes = result_data

            # Merge overlapping boxes
            boxes = merge_overlapping_boxes(boxes, iou_threshold=0.3)

            result = {
                'sample_id': i,
                'num_detections': len(boxes),
                'boxes': [b.to_dict() for b in boxes]
            }
            results.append(result)

            if args.visualize and (args.max_vis < 0 or i < args.max_vis):  # 2D visualization
                visualize_detections(
                    x, boxes, label,
                    output_path=f'detection_{i:03d}.png'
                )

            if args.visualize_3d and (args.max_vis < 0 or i < args.max_vis):  # 3D paper-style visualization
                # Try to get actual trajectory from dataset
                trajectory = None
                try:
                    import scipy.io as sio
                    rec = dataset.recordings[i]
                    mat = sio.loadmat(rec['event_path'], squeeze_me=True)
                    if 'Obj' in mat:
                        obj = mat['Obj']
                        if hasattr(obj, 'dtype') and obj.dtype.names:
                            trajectory = {
                                'x': np.asarray(obj['x']).flatten() if 'x' in obj.dtype.names else None,
                                'y': np.asarray(obj['y']).flatten() if 'y' in obj.dtype.names else None,
                                't': np.asarray(obj['ts']).flatten() if 'ts' in obj.dtype.names else None
                            }
                except Exception as e:
                    print(f"Warning: Could not load trajectory: {e}")

                visualize_3d_trajectory(
                    x, raw_spikes, label,
                    trajectory=trajectory,
                    output_path=f'figure4_sample_{i:03d}.png',
                    title=f'Sample {i}: Satellite Trajectory'
                )

            if args.animate_3d and (args.max_vis < 0 or i < args.max_vis):  # Animated 3D visualization
                # Load trajectory if not already loaded
                if trajectory is None:
                    try:
                        import scipy.io as sio
                        rec = dataset.recordings[i]
                        mat = sio.loadmat(rec['event_path'], squeeze_me=True)
                        if 'Obj' in mat:
                            obj = mat['Obj']
                            if hasattr(obj, 'dtype') and obj.dtype.names:
                                trajectory = {
                                    'x': np.asarray(obj['x']).flatten() if 'x' in obj.dtype.names else None,
                                    'y': np.asarray(obj['y']).flatten() if 'y' in obj.dtype.names else None,
                                    't': np.asarray(obj['ts']).flatten() if 'ts' in obj.dtype.names else None
                                }
                    except Exception as e:
                        print(f"Warning: Could not load trajectory for animation: {e}")

                animate_3d_trajectory(
                    x, raw_spikes, label,
                    trajectory=trajectory,
                    output_path=f'animation_sample_{i:03d}.gif',
                    title=f'Sample {i}: Satellite Detection (Animated)',
                    fps=args.animation_fps,
                    trail_length=args.animation_trail,
                )

            if args.tracking_video and (args.max_vis < 0 or i < args.max_vis):  # 2D tracking video with bounding boxes
                # Load trajectory if not already loaded
                if trajectory is None:
                    try:
                        import scipy.io as sio
                        rec = dataset.recordings[i]
                        mat = sio.loadmat(rec['event_path'], squeeze_me=True)
                        if 'Obj' in mat:
                            obj = mat['Obj']
                            if hasattr(obj, 'dtype') and obj.dtype.names:
                                trajectory = {
                                    'x': obj['x'] if 'x' in obj.dtype.names else None,
                                    'y': obj['y'] if 'y' in obj.dtype.names else None,
                                    't': obj['ts'] if 'ts' in obj.dtype.names else None
                                }
                    except Exception as e:
                        print(f"Warning: Could not load trajectory: {e}")

                create_tracking_video(
                    x, raw_spikes,
                    trajectory=trajectory,
                    label=label,
                    output_path=f'tracking_sample_{i:03d}.gif',
                    fps=args.animation_fps,
                )

            if args.trajectory_video and i < 10:  # HONEST trajectory video
                # Load trajectory if not already loaded
                if trajectory is None:
                    try:
                        import scipy.io as sio
                        rec = dataset.recordings[i]
                        mat = sio.loadmat(rec['event_path'], squeeze_me=True)
                        if 'Obj' in mat:
                            obj = mat['Obj']
                            if hasattr(obj, 'dtype') and obj.dtype.names:
                                trajectory = {
                                    'x': obj['x'] if 'x' in obj.dtype.names else None,
                                    'y': obj['y'] if 'y' in obj.dtype.names else None,
                                    't': obj['ts'] if 'ts' in obj.dtype.names else None
                                }
                    except Exception as e:
                        print(f"Warning: Could not load trajectory: {e}")

                print(f"\nSample {i}: Generating HONEST trajectory video...")
                create_trajectory_video(
                    x, raw_spikes,
                    trajectory=trajectory,
                    label=label,
                    output_path=f'trajectory_sample_{i:03d}.gif',
                    fps=args.animation_fps,
                )

            if args.hulk_tracking and (args.max_vis < 0 or i < args.max_vis):  # HULK/SMASH instance segmentation
                print(f"\nSample {i}: Running HULK/SMASH tracking...")
                hulk_result = run_hulk_smash_tracking(
                    model, x, device,
                    previous_objects=previous_objects,  # Link to previous for tracking!
                    inference_mode=args.inference_mode
                )

                print(f"  Classification spikes: {hulk_result['n_spikes']}")
                print(f"  Instances found: {len(hulk_result['instances'])}")
                print(f"  Objects after grouping: {len(hulk_result['objects'])}")

                # Show cross-sequence matches
                if hulk_result['matches']:
                    print(f"  Cross-sequence matches: {hulk_result['matches']}")
                    for curr_id, prev_id in hulk_result['matches'].items():
                        print(f"    Object {curr_id} â† matched to previous Object {prev_id}")
                elif previous_objects:
                    print(f"  No matches to previous {len(previous_objects)} objects")

                if hulk_result['objects']:
                    for obj in hulk_result['objects']:
                        print(f"    Object {obj.object_id}: {obj.n_instances} instances, bbox={obj.combined_bbox}")

                # Update tracking state for next iteration
                previous_objects = hulk_result['objects'] if hulk_result['objects'] else None

                # Track history
                track_history.append({
                    'sample': i,
                    'n_objects': len(hulk_result['objects']),
                    'matches': hulk_result['matches']
                })

                # Add to results
                result['hulk'] = {
                    'n_spikes': hulk_result['n_spikes'],
                    'n_instances': len(hulk_result['instances']),
                    'n_objects': len(hulk_result['objects']),
                    'matches': hulk_result['matches'],
                    'objects': [
                        {
                            'id': obj.object_id,
                            'n_instances': obj.n_instances,
                            'bbox': obj.combined_bbox.to_xyxy() if obj.combined_bbox else None
                        }
                        for obj in hulk_result['objects']
                    ]
                }

            if (i + 1) % 10 == 0:
                print(f"Processed {i + 1}/{len(dataset)}")

        print(f"\nTotal detections: {sum(r['num_detections'] for r in results)}")

        # Print HULK/SMASH tracking summary
        if args.hulk_tracking and track_history:
            total_matches = sum(len(h['matches']) for h in track_history)
            print(f"\n=== HULK/SMASH Tracking Summary ===")
            print(f"  Samples processed: {len(track_history)}")
            print(f"  Total cross-sequence matches: {total_matches}")
            if total_matches > 0:
                print(f"  Tracking successful: Objects matched across sequences!")

    elif args.input:
        # Single file inference
        import scipy.io as sio

        mat = sio.loadmat(args.input, squeeze_me=True)
        # Extract events - adapt based on your .mat format
        if 'TD' in mat:
            td = mat['TD']
            # Convert to tensor format...
            print("Single file inference not fully implemented - use --data-root")
        else:
            print(f"Unknown .mat format. Keys: {list(mat.keys())}")

    # Save results
    with open(args.output, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"Results saved: {args.output}")


if __name__ == '__main__':
    main()
