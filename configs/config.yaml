# =============================================================================
# SpikeSEG Training Configuration
# =============================================================================
#
# Default configuration for STDP-based training of SpikeSEG.
# 
# Paper References:
#   - Kheradpisheh et al. 2018 - "STDP-based spiking deep CNNs"
#   - Kirkland et al. 2023 (IGARSS) - "Neuromorphic sensing for space domain awareness"
#
# Usage:
#   python -m spikeseg.training.train --config configs/default.yaml
#
# =============================================================================

# -----------------------------------------------------------------------------
# Experiment Settings
# -----------------------------------------------------------------------------
experiment_name: "spikeseg_default"
output_dir: "./runs"
seed: 42
device: "cuda"  # "cuda", "cuda:0", "cpu"

# -----------------------------------------------------------------------------
# Training Duration
# -----------------------------------------------------------------------------
max_epochs: 50              # More epochs for STDP convergence
max_samples_per_epoch: null  # null = use full dataset (67 samples with sensor=all)

# Layer-wise training control
train_conv1: false  # Conv1 uses fixed DoG/Gabor filters (no STDP)
train_conv2: true   # Train Conv2 via STDP
train_conv3: true   # Train Conv3 via STDP

# Memory optimization
clear_cache_interval: 1000  # Clear CUDA cache every N samples
gradient_checkpointing: false  # Not used for STDP

# -----------------------------------------------------------------------------
# STDP Learning Parameters
# -----------------------------------------------------------------------------
# IGARSS 2023 (Kirkland et al.): α⁺ = 0.04, α⁻ = 0.03 (10x higher for faster convergence)
# Note: Kheradpisheh 2018 uses 0.004/0.003 - use those for exact paper replication
stdp:
  lr_plus: 0.04           # LTP learning rate (potentiation) - IGARSS 2023
  lr_minus: 0.03          # LTD learning rate (depression) - IGARSS 2023
  weight_min: 0.0         # Minimum weight value
  weight_max: 1.0         # Maximum weight value
  weight_init_mean: 0.8   # Weight initialization mean
  weight_init_std: 0.01   # Weight initialization std (IGARSS 2023: 0.01)
  use_soft_bounds: true   # Use multiplicative STDP: Δw = α·w·(1-w)

# -----------------------------------------------------------------------------
# Winner-Take-All (WTA) Parameters
# -----------------------------------------------------------------------------
# Paper uses BOTH global (intra-map) AND local (inter-map) competition
wta:
  mode: "both"            # "global", "local", or "both" - paper uses both
  local_radius: 2         # Radius for local inhibition
  enable_homeostasis: true
  target_rate: 0.1        # Target firing rate for homeostasis
  homeostasis_lr: 0.001   # Threshold adaptation rate
  threshold_min: 1.0      # Minimum adaptive threshold
  threshold_max: 100.0    # Maximum adaptive threshold

# -----------------------------------------------------------------------------
# Adaptive Thresholds (Homeostasis / PENT)
# -----------------------------------------------------------------------------
# IGARSS 2023 uses ADAPTIVE thresholds (PENT mechanism)
# "In the event of a post-neuronal spike, we uniformly increase the firing
# threshold of all the post-units constituting the feature map."
homeostasis:
  enabled: true           # CRITICAL: Must be true for IGARSS 2023 / EBSSA
  theta_rest: 0.1         # Resting threshold (should match base threshold)
  theta_plus: 0.02        # Stronger threshold increase (20% of base) to penalize winners faster
  tau_theta: 500.0        # Faster decay to let suppressed neurons recover
  theta_max: 10.0         # Much higher cap (100x base) to really suppress dominant neurons

  # Dead neuron recovery
  dead_neuron_recovery: true
  dead_threshold: 0.01    # Firing rate below this = dead neuron
  recovery_boost: 0.1     # Weight perturbation magnitude for recovery

# -----------------------------------------------------------------------------
# Convergence Monitoring
# -----------------------------------------------------------------------------
convergence:
  min_wins_per_neuron: 2    # Min WTA wins for neuron to be "converged" (very low for small data)
  target_ratio: 0.40        # Target fraction of converged neurons (50% realistic for 67 samples)
  patience: 20              # Epochs without improvement before stopping (very patient)
  delta_threshold: 0.0001   # Weight change threshold for convergence
  check_interval: 100       # Check convergence every N samples

# -----------------------------------------------------------------------------
# Data Loading
# -----------------------------------------------------------------------------
data:
  dataset: "ebssa"          # Dataset name
  data_root: "../ebssa-data-utah/ebssa"  # Data directory (contains 'Labelled Data' folder)
  sensor: "all"             # Use ALL sensors (ATIS + DAVIS) for more data
  include_unlabelled: false  # Disabled - unlabelled data has incompatible format
  batch_size: 1             # Batch size (typically 1 for online STDP)
  num_workers: 0            # Single-threaded to avoid OOM with spatial filtering
  pin_memory: true          # Pin memory for faster GPU transfer
  
  # Event/spike parameters (SpikeSEG: 20 timesteps for events)
  timestep_ms: 100.0        # Simulation timestep in milliseconds
  n_timesteps: 20           # Number of timesteps per sample (10 event + 10 propagation)
  input_height: 128         # Input height
  input_width: 128          # Input width
  input_channels: 2         # Input channels (2 for ON/OFF polarity)
  
  # Processing
  normalize: true           # Normalize event counts
  shuffle_train: true       # Shuffle training data
  shuffle_val: false        # Shuffle validation data
  train_ratio: 1.0          # Use ALL data for training (STDP is unsupervised)
  
  # Sliding window sampling - disabled since we filter to object time window
  # Object-time filtering extracts only the ~0.5-2s when satellite is tracked
  windows_per_recording: 1   # One sample per recording (object time window)
  window_overlap: 0.0        # Not used with windows_per_recording=1
  
  # Data augmentation (safe options - no cropping to preserve edge satellites)
  augmentation:
    enabled: true
    flip_horizontal: true   # Safe: satellites can appear on either side
    flip_polarity: true     # Safe: doesn't change spatial location
    drop_rate: 0.1          # Safe: randomly drop 10% of events
    noise_rate: 0.05        # Safe: add 5% noise events
    # random_crop: null     # DISABLED: would cut off edge satellites

# -----------------------------------------------------------------------------
# Model Architecture
# -----------------------------------------------------------------------------
# IGARSS 2023 / EBSSA: 4 features in layer 1, 36 features in layer 2
# Kernel sizes: 5×5 for Conv1/Conv2, 7×7 for Conv3
model:
  n_classes: 2              # Number of output classes (2 for EBSSA binary segmentation)
  conv1_channels: 4         # Conv1 output channels
  conv2_channels: 36        # Conv2 output channels
  kernel_sizes: [5, 5, 7]   # Kernel sizes for Conv1, Conv2, Conv3

  # Pooling configuration (IGARSS 2023: standard 2×2 pooling)
  # Note: Kheradpisheh 2018 uses 7×7 stride 6 for Pool1
  pool1_kernel: 2
  pool1_stride: 2
  pool2_kernel: 2
  pool2_stride: 2

  # Layer thresholds and leaks (IGARSS 2023)
  # Paper: "λ is set to 90% and 10% of the neuron threshold in layers 1 and 2"
  # NOTE: For VERY sparse EBSSA data, thresholds must be very low!
  # - Sparse events + DoG filters give output ~0.1-0.5 per timestep
  # - With 90% leak, need threshold low enough that leak << input
  # - threshold=0.1, leak=0.09 → can accumulate with input ~0.1
  thresholds: [0.1, 0.1, 0.1]   # Very low thresholds for very sparse events
  leaks: [0.09, 0.01, 0.0]      # Leak values: 90%, 10%, 0% of threshold

  # Initialization
  use_dog_filters: true     # Use Difference-of-Gaussians for Conv1

# -----------------------------------------------------------------------------
# Checkpointing
# -----------------------------------------------------------------------------
checkpoint:
  save_dir: "checkpoints"   # Subdirectory for checkpoints
  save_interval: 1          # Save every N epochs
  keep_last_n: 3            # Keep last N checkpoints
  save_best: true           # Save best model separately
  save_on_interrupt: true   # Save on keyboard interrupt

# -----------------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------------
logging:
  log_dir: "logs"           # Subdirectory for logs
  log_level: "INFO"         # Logging level
  log_interval: 100         # Log every N samples
  tensorboard: true         # Enable TensorBoard logging
  wandb: false              # Enable Weights & Biases
  wandb_project: "spikeseg" # W&B project name
  print_model_summary: true # Print model summary at start